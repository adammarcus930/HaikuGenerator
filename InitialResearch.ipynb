{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from nltk.corpus import cmudict\n",
    "import copy\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "%load_ext line_profiler\n",
    "cmudict = cmudict.dict()\n",
    "# Load pre-trained BERT model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Functions to Assist in Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into words\n",
    "def getwords(text):\n",
    "    words = pd.Series(re.findall(r\"[\\w']+\", text))\n",
    "    return words\n",
    "# Split text into individual haikus\n",
    "def gethaikus(text):\n",
    "    haikus = pd.Series(text.split(\"\\n\\n\"))\n",
    "    return haikus\n",
    "# Convert an array of words to a single string\n",
    "def arraytotext(arr):\n",
    "    text = ' '.join(arr)\n",
    "    return text\n",
    "# Find words that are not located in the cmudict phoenetic dictionary\n",
    "def getunknowns(words):\n",
    "    unknown_words = np.array([word for word in words if word.lower() not in cmudict.keys()])\n",
    "    return unknown_words\n",
    "# Gets frequency of word use in a given list of words\n",
    "def wordcount(unknown_words):\n",
    "    prob_word_freq = pd.Series(Counter(unknown_words)).sort_values(ascending=False)\n",
    "    return prob_word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Haikus in the Same Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source 1\n",
    "text = open('Haikus/haikuzao.txt', 'r').read()\n",
    "text = text.lower()\n",
    "haikus = gethaikus(text)\n",
    "# Source 2\n",
    "gutenberg = pd.read_csv('Haikus/gutenberg.csv')\n",
    "haikus = haikus.append(pd.Series(gutenberg['haiku']).apply(lambda x: x.lower()))\n",
    "# Source 3\n",
    "modern_renaissance = pd.read_csv('Haikus/modern_renaissance.csv')\n",
    "# make lower case and ensure that the new line notation is the same\n",
    "modern_renaissance = pd.Series(modern_renaissance['content']).apply(lambda x: x.lower().replace(\"\\r\\n\", \"\\n\"))\n",
    "haikus = haikus.append(modern_renaissance)\n",
    "# Source 4\n",
    "sballas = pd.read_csv('Haikus/sballas8.csv', header=None)\n",
    "haikus = haikus.append(pd.Series(sballas[0]))\n",
    "# Source 5\n",
    "temps = pd.read_csv('Haikus/tempslibres.csv', encoding = \"ISO-8859-1\")\n",
    "# Only English\n",
    "temps = temps[temps['lang']=='en']\n",
    "# make lower case and ensure that the new line notation is the same\n",
    "haikus = haikus.append(pd.Series(temps['haiku']).apply(lambda x: x.lower().replace(\"\\r\\n\", \"\\n\")))\n",
    "# Source 6\n",
    "hjson = pd.read_json('Haikus/unim_poem.json')\n",
    "haikus = haikus.append(pd.Series(hjson['poem']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "an'       2419\n",
       "o'er      2041\n",
       "sg        1421\n",
       "wi'        879\n",
       "sc         646\n",
       "'          614\n",
       "pl         591\n",
       "a'         564\n",
       "acc        533\n",
       "'t         500\n",
       "ii         479\n",
       "iii        417\n",
       "th'        403\n",
       "sae        379\n",
       "_          378\n",
       "nbsp       358\n",
       "iv         292\n",
       "honour     277\n",
       "nought     275\n",
       "tho'       272\n",
       "nae        264\n",
       "thro'      259\n",
       "aught      248\n",
       "hae        246\n",
       "e'er       243\n",
       "beheld     223\n",
       "frae       215\n",
       "e'en       213\n",
       "canst      211\n",
       "quoth      208\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words from haikus and determine whih ones do not exist in \n",
    "text = arraytotext(haikus)\n",
    "unknown_words = getunknowns(getwords(text))\n",
    "wordcount(unknown_words).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted characters\n",
    "def cleanwords(text):    \n",
    "    #Clean dashes\n",
    "    text = text.replace('-',\" \")\n",
    "    #Clean apostrophe\n",
    "    text = text.replace('\\'',\"\")\n",
    "    # Clean numbers\n",
    "    words = getwords(text)\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            text = re.sub(rf'\\b{word}\\b',num2words(word),text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython/7.5.0/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Clean Haikus\n",
    "haikus = haikus.apply(cleanwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Words to Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert plain text to Phoenetic\n",
    "def getphoneme(word):\n",
    "    phoneme = ''.join(cmudict[word][0]) # always use first pronuciations at index 0\n",
    "    return phoneme\n",
    "\n",
    "# Finds unknown words that can be split into two words\n",
    "def findwordsplits(unknown_words):\n",
    "    splitwords = {}\n",
    "    for word in unknown_words:\n",
    "        wordlength = len(word)\n",
    "        for i in range(wordlength):\n",
    "            split1 = word[0:i+1]\n",
    "            split2 = word[i+1:wordlength]\n",
    "\n",
    "            if split1 in cmudict.keys():\n",
    "                if split2 in cmudict.keys():\n",
    "                    splitwords[word] = [split1,split2]      \n",
    "    return splitwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lun', 'e']\n",
      "['sleet', 'y']\n",
      "['ko', 'i']\n",
      "['mis', 'o']\n",
      "['a', 'kora']\n",
      "['fur', 'u']\n",
      "['abet', 'e']\n",
      "['kon', 'u']\n",
      "['y', 'ori']\n",
      "['hit', 'o']\n",
      "['haig', 'a']\n",
      "['nap', 'e']\n",
      "['bash', 'o']\n",
      "['criss', 'e']\n",
      "['en', 'e']\n",
      "['tomb', 'e']\n",
      "['i', 'pod']\n",
      "['w', 'i']\n",
      "['ham', 'e']\n",
      "['inca', 'a']\n",
      "['stern', 'y']\n",
      "['stam', 'e']\n",
      "['a', 'hint']\n",
      "['syn', 'e']\n",
      "['both', 'y']\n",
      "['agan', 'e']\n",
      "['o', 'ot']\n",
      "['who', 'o']\n",
      "['sare', 'e']\n",
      "['couch', 'e']\n",
      "['brum', 'e']\n",
      "['burk', 'a']\n",
      "['night', 'i']\n",
      "['delayed', 'a']\n",
      "['mint', 'y']\n",
      "['napp', 'y']\n",
      "['de', 'i']\n",
      "['mort', 'o']\n",
      "['plumeri', 'a']\n",
      "['chill', 'i']\n",
      "['dusk', 'y']\n",
      "['kim', 'i']\n",
      "['b', 'a']\n",
      "['i', 'wa']\n",
      "['ot', 'o']\n",
      "['a', 'ah']\n",
      "['um', 'e']\n",
      "['iwan', 'u']\n",
      "['ter', 'a']\n",
      "['ash', 'i']\n",
      "['kyo', 'o']\n",
      "['yuk', 'i']\n",
      "['kam', 'i']\n",
      "['har', 'u']\n",
      "['brenn', 'e']\n",
      "['corell', 'i']\n",
      "['ruck', 'e']\n",
      "['sec', 'u']\n",
      "['familial', 'e']\n",
      "['c', 'e']\n",
      "['tracer', 'y']\n",
      "['suon', 'o']\n",
      "['sill', 'a']\n",
      "['aspirin', 'e']\n",
      "['i', 'cycles']\n",
      "['slat', 'y']\n",
      "['moss', 'y']\n",
      "['draught', 'y']\n",
      "['gess', 'o']\n",
      "['i', 'i']\n",
      "['o', 'dour']\n",
      "['dull', 'y']\n",
      "['ta', 'u']\n",
      "['troche', 'e']\n",
      "['soldier', 'y']\n",
      "['a', 'flare']\n",
      "['finn', 'y']\n",
      "['veer', 'y']\n",
      "['briar', 'y']\n",
      "['a', 'flush']\n",
      "['a', 'right']\n",
      "['crees', 'e']\n",
      "['pulp', 'y']\n",
      "['u', 'surer']\n",
      "['chant', 'y']\n",
      "['beam', 'y']\n",
      "['i', 'v']\n",
      "['berth', 'e']\n",
      "['a', 'whirl']\n",
      "['shrill', 'y']\n",
      "['baron', 'y']\n",
      "['i', 'en']\n",
      "['i', 'eh']\n",
      "['chia', 'o']\n",
      "['ya', 'i']\n",
      "['i', 'u']\n",
      "['a', 'o']\n",
      "['leth', 'e']\n",
      "['task', 'e']\n",
      "['mean', 'e']\n",
      "['a', 'reeds']\n",
      "['help', 'e']\n",
      "['chief', 'e']\n",
      "['perform', 'e']\n",
      "['weak', 'e']\n",
      "['imp', 'e']\n",
      "['mild', 'e']\n",
      "['goddess', 'e']\n",
      "['think', 'e']\n",
      "['hear', 'e']\n",
      "['plain', 'e']\n",
      "['shield', 'e']\n",
      "['deep', 'e']\n",
      "['remain', 'e']\n",
      "['field', 'e']\n",
      "['steed', 'e']\n",
      "['curb', 'e']\n",
      "['dear', 'e']\n",
      "['sweet', 'e']\n",
      "['deed', 'e']\n",
      "['cheer', 'e']\n",
      "['seem', 'e']\n",
      "['solemn', 'e']\n",
      "['queen', 'e']\n",
      "['earn', 'e']\n",
      "['learn', 'e']\n",
      "['ass', 'e']\n",
      "['black', 'e']\n",
      "['western', 'e']\n",
      "['foul', 'e']\n",
      "['farr', 'e']\n",
      "['dwarf', 'e']\n",
      "['storm', 'e']\n",
      "['pour', 'e']\n",
      "['vi', 'i']\n",
      "['seek', 'e']\n",
      "['y', 'clad']\n",
      "['starr', 'e']\n",
      "['worn', 'e']\n",
      "['scorn', 'e']\n",
      "['prays', 'e']\n",
      "['elm', 'e']\n",
      "['oak', 'e']\n",
      "['aspin', 'e']\n",
      "['cypress', 'e']\n",
      "['i', 'x']\n",
      "['forlorn', 'e']\n",
      "['e', 'ugh']\n",
      "['overblown', 'e']\n",
      "['return', 'e']\n",
      "['shown', 'e']\n",
      "['unknown', 'e']\n",
      "['own', 'e']\n",
      "['seen', 'e']\n",
      "['find', 'e']\n",
      "['needless', 'e']\n",
      "['xi', 'i']\n",
      "['mischief', 'e']\n",
      "['self', 'e']\n",
      "['staid', 'e']\n",
      "['half', 'e']\n",
      "['retain', 'e']\n",
      "['disdain', 'e']\n",
      "['tail', 'e']\n",
      "['soon', 'e']\n",
      "['entrail', 'e']\n",
      "['turn', 'e']\n",
      "['again', 'e']\n",
      "['darkness', 'e']\n",
      "['elf', 'e']\n",
      "['train', 'e']\n",
      "['vain', 'e']\n",
      "['endless', 'e']\n",
      "['grief', 'e']\n",
      "['constrain', 'e']\n",
      "['slack', 'e']\n",
      "['lack', 'e']\n",
      "['a', 'egyptian']\n",
      "['stink', 'e']\n",
      "['shrink', 'e']\n",
      "['sink', 'e']\n",
      "['spawn', 'e']\n",
      "['ink', 'e']\n",
      "['flock', 'e']\n",
      "['certain', 'e']\n",
      "['mind', 'e']\n",
      "['stream', 'e']\n",
      "['fear', 'e']\n",
      "['devour', 'e']\n",
      "['drunk', 'e']\n",
      "['plam', 'e']\n",
      "['feet', 'e']\n",
      "['book', 'e']\n",
      "['warr', 'e']\n",
      "['near', 'e']\n",
      "['wear', 'e']\n",
      "['wilderness', 'e']\n",
      "['distress', 'e']\n",
      "['bait', 'e']\n",
      "['work', 'e']\n",
      "['fro', 'e']\n",
      "['hermit', 'e']\n",
      "['morn', 'e']\n",
      "['look', 'e']\n",
      "['sleep', 'e']\n",
      "['two', 'o']\n",
      "['peep', 'e']\n",
      "['repair', 'e']\n",
      "['steep', 'e']\n",
      "['keep', 'e']\n",
      "['lull', 'e']\n",
      "['wind', 'e']\n",
      "['sown', 'e']\n",
      "['careless', 'e']\n",
      "['shook', 'e']\n",
      "['dream', 'e']\n",
      "['brain', 'e']\n",
      "['break', 'e']\n",
      "['stubborn', 'e']\n",
      "['devoid', 'e']\n",
      "['a', 'fore']\n",
      "['y', 'vie']\n",
      "['clean', 'e']\n",
      "['shameless', 'e']\n",
      "['himself', 'e']\n",
      "['weep', 'e']\n",
      "['li', 'i']\n",
      "['kingdom', 'e']\n",
      "['deem', 'e']\n",
      "['weariness', 'e']\n",
      "['toss', 'e']\n",
      "['obtain', 'e']\n",
      "['gain', 'e']\n",
      "['kiss', 'e']\n",
      "['feel', 'e']\n",
      "['air', 'e']\n",
      "['hon', 'y']\n",
      "['sinn', 'e']\n",
      "['quick', 'e']\n",
      "['f', 'a']\n",
      "['carow', 'e']\n",
      "['med', 'e']\n",
      "['nothing', 'e']\n",
      "['redress', 'e']\n",
      "['ground', 'e']\n",
      "['toward', 'e']\n",
      "['behold', 'e']\n",
      "['cold', 'e']\n",
      "['wold', 'e']\n",
      "['pang', 'e']\n",
      "['fret', 'e']\n",
      "['bet', 'e']\n",
      "['wann', 'e']\n",
      "['blew', 'e']\n",
      "['he', 'u']\n",
      "['fend', 'e']\n",
      "['hound', 'e']\n",
      "['bound', 'e']\n",
      "['now', 'e']\n",
      "['fel', 'e']\n",
      "['troy', 'e']\n",
      "['pret', 'y']\n",
      "['oft', 'e']\n",
      "['soft', 'e']\n",
      "['saw', 'e']\n",
      "['wasp', 'e']\n",
      "['lord', 'e']\n",
      "['adorn', 'e']\n",
      "['mourn', 'e']\n",
      "['rays', 'e']\n",
      "['beam', 'e']\n",
      "['damp', 'e']\n",
      "['wiz', 'e']\n",
      "['silk', 'e']\n",
      "['bridal', 'e']\n",
      "['e', 'eke']\n",
      "['mull', 'a']\n",
      "['rush', 'y']\n",
      "['heer', 'e']\n",
      "['deck', 'e']\n",
      "['year', 'e']\n",
      "['between', 'e']\n",
      "['ara', 'y']\n",
      "['bowl', 'e']\n",
      "['cream', 'e']\n",
      "['neck', 'e']\n",
      "['upp', 'e']\n",
      "['oba', 'y']\n",
      "['anthem', 'e']\n",
      "['answer', 'e']\n",
      "['altar', 'e']\n",
      "['lend', 'e']\n",
      "['gloom', 'e']\n",
      "['appear', 'e']\n",
      "['calm', 'e']\n",
      "['a', 'fray']\n",
      "['stork', 'e']\n",
      "['womb', 'e']\n",
      "['inform', 'e']\n",
      "['stain', 'e']\n",
      "['possess', 'e']\n",
      "['happiness', 'e']\n",
      "['ph', 'a']\n",
      "['a', 'main']\n",
      "['day', 'o']\n",
      "['rutt', 'y']\n",
      "['brick', 'y']\n",
      "['greet', 'e']\n",
      "['lass', 'e']\n",
      "['a', 'downe']\n",
      "['cheek', 'e']\n",
      "['quench', 'e']\n",
      "['much', 'e']\n",
      "['ladd', 'e']\n",
      "['whom', 'e']\n",
      "['forswear', 'e']\n",
      "['y', 's']\n",
      "['southern', 'e']\n",
      "['dart', 'e']\n",
      "['start', 'e']\n",
      "['glenn', 'e']\n",
      "['record', 'e']\n",
      "['sex', 'e']\n",
      "['she', 'e']\n",
      "['blemish', 'e']\n",
      "['grassi', 'e']\n",
      "['damask', 'e']\n",
      "['o', 'e']\n",
      "['redd', 'e']\n",
      "['cher', 'e']\n",
      "['broad', 'e']\n",
      "['below', 'e']\n",
      "['show', 'e']\n",
      "['brightness', 'e']\n",
      "['overthrow', 'e']\n",
      "['shew', 'e']\n",
      "['seed', 'e']\n",
      "['breed', 'e']\n",
      "['heed', 'e']\n",
      "['speed', 'e']\n",
      "['soot', 'e']\n",
      "['y', 'even']\n",
      "['reign', 'e']\n",
      "['princess', 'e']\n",
      "['bind', 'e']\n",
      "['fast', 'e']\n",
      "['strow', 'e']\n",
      "['rys', 'e']\n",
      "['thank', 'e']\n",
      "['emblem', 'e']\n",
      "['heavy', 'e']\n",
      "['lead', 'e']\n",
      "['less', 'e']\n",
      "['strain', 'e']\n",
      "['feed', 'e']\n",
      "['pris', 'e']\n",
      "['beat', 'e']\n",
      "['restrain', 'e']\n",
      "['lawless', 'e']\n",
      "['prick', 'e']\n",
      "['grain', 'e']\n",
      "['sky', 'e']\n",
      "['clown', 'e']\n",
      "['cuddy', 'e']\n",
      "['reed', 'e']\n",
      "['ear', 'e']\n",
      "['dred', 'e']\n",
      "['clay', 'e']\n",
      "['y', 'goe']\n",
      "['dread', 'e']\n",
      "['stoup', 'e']\n",
      "['shoot', 'e']\n",
      "['swann', 'e']\n",
      "['climb', 'e']\n",
      "['hi', 'e']\n",
      "['ski', 'e']\n",
      "['compass', 'e']\n",
      "['weighty', 'e']\n",
      "['throw', 'e']\n",
      "['threat', 'e']\n",
      "['meat', 'e']\n",
      "['fruit', 'e']\n",
      "['sweat', 'e']\n",
      "['rear', 'e']\n",
      "['teach', 'e']\n",
      "['bellon', 'a']\n",
      "['warm', 'e']\n",
      "['charm', 'e']\n",
      "['farm', 'e']\n",
      "['ill', 'o']\n",
      "['freak', 'e']\n",
      "['mirth', 'e']\n",
      "['control', 'e']\n",
      "['so', 'e']\n",
      "['hues', 'e']\n",
      "['lou', 'e']\n",
      "['rosalind', 'e']\n",
      "['lastly', 'e']\n",
      "['clout', 'e']\n",
      "['y', 'pent']\n",
      "['fold', 'e']\n",
      "['sheep', 'e']\n",
      "['shep', 'e']\n",
      "['abou', 'e']\n",
      "['smart', 'e']\n",
      "['y', 't']\n",
      "['moss', 'e']\n",
      "['leaf', 'e']\n",
      "['blossom', 'e']\n",
      "['blown', 'e']\n",
      "['witness', 'e']\n",
      "['tenn', 'e']\n",
      "['bless', 'e']\n",
      "['io', 'y']\n",
      "['avail', 'e']\n",
      "['wain', 'e']\n",
      "['fi', 'e']\n",
      "['witt', 'a']\n",
      "['window', 'y']\n",
      "['clim', 'e']\n",
      "['sous', 'e']\n",
      "['jap', 'e']\n",
      "['town', 'y']\n",
      "['a', 'gue']\n",
      "['steep', 'y']\n",
      "['op', 'e']\n",
      "['pos', 'y']\n",
      "['mops', 'a']\n",
      "['greif', 'e']\n",
      "['fain', 'e']\n",
      "['il', 'e']\n",
      "['despair', 'e']\n",
      "['embalm', 'e']\n",
      "['entomb', 'e']\n",
      "['doom', 'e']\n",
      "['draw', 'e']\n",
      "['neer', 'e']\n",
      "['claim', 'e']\n",
      "['freedom', 'e']\n",
      "['deign', 'e']\n",
      "['food', 'e']\n",
      "['wail', 'e']\n",
      "['clear', 'e']\n",
      "['lurk', 'e']\n",
      "['spark', 'e']\n",
      "['frail', 'e']\n",
      "['dross', 'y']\n",
      "['spoil', 'e']\n",
      "['contain', 'e']\n",
      "['sheen', 'e']\n",
      "['hour', 'e']\n",
      "['beggar', 'y']\n",
      "['swear', 'e']\n",
      "['cheap', 'e']\n",
      "['con', 'y']\n",
      "['liv', 'y']\n",
      "['pod', 'e']\n",
      "['y', 'wis']\n",
      "['f', 'y']\n",
      "['o', 'er']\n",
      "['red', 'e']\n",
      "['recur', 'e']\n",
      "['gram', 'e']\n",
      "['rage', 'i']\n",
      "['place', 'i']\n",
      "['word', 'o']\n",
      "['preas', 'e']\n",
      "['minstrels', 'y']\n",
      "['letters', 'i']\n",
      "['franz', 'y']\n",
      "['i', 'wis']\n",
      "['wist', 'e']\n",
      "['tanger', 'e']\n",
      "['woman', 'a']\n",
      "['o', 'villers']\n",
      "['blight', 'y']\n",
      "['y', 'pres']\n",
      "['pieri', 'a']\n",
      "['bella', 'y']\n",
      "['hymn', 'i']\n",
      "['pine', 'y']\n",
      "['chalk', 'y']\n",
      "['palm', 'y']\n",
      "['your', 'e']\n",
      "['blood', 'i']\n",
      "['waiting', 'i']\n",
      "['syring', 'a']\n",
      "['atmosphere', 'a']\n",
      "['a', 'bristle']\n",
      "['still', 'y']\n",
      "['lover', 'i']\n",
      "['poetic', 'a']\n",
      "['stan', 'e']\n",
      "['o', 'clock']\n",
      "['e', 'z']\n",
      "['y', \"o'\"]\n",
      "['e', 'f']\n",
      "['y', 'allah']\n",
      "['gwin', 'e']\n",
      "['o', 'b']\n",
      "['tho', 'o']\n",
      "['chun', 'e']\n",
      "['once', 'o']\n",
      "['normand', 'e']\n",
      "['papp', 'y']\n",
      "['house', 'y']\n",
      "['maz', 'y']\n",
      "['pong', 'a']\n",
      "['rim', 'u']\n",
      "['tu', 'i']\n",
      "['pik', 'a']\n",
      "['hallow', 'e']\n",
      "['sum', 'i']\n",
      "['mug', 'o']\n",
      "['fad', 'o']\n",
      "['blows', 'y']\n",
      "['adz', 'e']\n",
      "['as', 'o']\n",
      "['tomb', 'o']\n",
      "['winter', 'y']\n",
      "['madron', 'a']\n",
      "['mach', 'u']\n",
      "['sob', 'a']\n",
      "['nash', 'i']\n",
      "['wad', 'i']\n",
      "['hepatic', 'a']\n",
      "['tang', 'y']\n",
      "['a', 'waking']\n",
      "['a', 'stir']\n",
      "['hack', 'y']\n",
      "['kahl', 'o']\n",
      "['ul', 'u']\n",
      "['pest', 'o']\n",
      "['hurl', 'e']\n",
      "['un', 'e']\n",
      "['o', 'or']\n",
      "['tassi', 'e']\n",
      "['na', 'e']\n",
      "['peko', 'e']\n",
      "['a', 'leppo']\n",
      "['freest', 'y']\n",
      "['y', 'ill']\n",
      "['y', 'alla']\n",
      "[\"'m\", 'e']\n",
      "['y', 'ince']\n",
      "['rosh', 'i']\n",
      "['wa', 'e']\n",
      "['ha', 'e']\n",
      "['on', 'y']\n",
      "['gyr', 'e']\n",
      "['coffin', 'i']\n",
      "['breaking', 'i']\n",
      "['hysteric', 'a']\n",
      "['uss', 'e']\n",
      "['pang', 'a']\n",
      "['k', 'e']\n",
      "['th', 'a']\n",
      "['ali', 'a']\n",
      "['a', 'fora']\n",
      "['spirit', 'u']\n",
      "['tu', 'o']\n",
      "['u', 't']\n",
      "['karn', 'a']\n",
      "['pepper', 'y']\n",
      "['sor', 'y']\n",
      "['sup', 'a']\n",
      "['v', 'a']\n",
      "['frein', 'e']\n",
      "['tant', 'o']\n",
      "['salm', 'a']\n",
      "['wreath', 'e']\n",
      "[\"'m\", 'y']\n",
      "['tal', 'i']\n",
      "['do', 'y']\n",
      "['tod', 'o']\n",
      "['herman', 'o']\n",
      "['pech', 'o']\n",
      "['sec', 'a']\n",
      "['vomit', 'o']\n",
      "['a', 'ether']\n",
      "['nit', 'y']\n",
      "['tan', 'a']\n",
      "['i', 'z']\n",
      "['expect', 'o']\n",
      "['ill', 'e']\n",
      "['sa', 'e']\n",
      "['gan', 'e']\n",
      "['cann', 'a']\n",
      "['boni', 'e']\n",
      "['breasts', 'i']\n",
      "['tull', 'e']\n",
      "['ciel', 'o']\n",
      "['and', 'a']\n",
      "['op', 'u']\n",
      "['mark', 'u']\n",
      "['lam', 'u']\n",
      "['o', 'siers']\n",
      "['a', 'f']\n",
      "['dig', 'o']\n",
      "['respond', 'e']\n",
      "['pillow', 'y']\n",
      "['mort', 'e']\n",
      "['moment', 'o']\n",
      "['fu', 'e']\n",
      "['summer', 'y']\n",
      "['orand', 'o']\n",
      "['god', 'o']\n",
      "['dunc', 'e']\n",
      "['i', 'gnu']\n",
      "['pal', 'i']\n",
      "['cops', 'e']\n",
      "['punjab', 'i']\n",
      "['eir', 'e']\n",
      "['mor', 'u']\n",
      "[\"i's\", 'e']\n",
      "['pais', 'e']\n",
      "['a', 'based']\n",
      "['pallid', 'i']\n",
      "['pers', 'i']\n",
      "['odor', 'e']\n",
      "['mi', 'o']\n",
      "['i', 'ying']\n",
      "['luc', 'i']\n",
      "['him', 'a']\n",
      "['hour', 'i']\n",
      "['present', 'e']\n",
      "['teng', 'o']\n",
      "['exist', 'a']\n",
      "['cas', 'i']\n",
      "['duet', 'o']\n",
      "['chatter', 'y']\n",
      "['spleen', 'y']\n",
      "['iridescent', 'i']\n",
      "['musich', 'e']\n",
      "['set', 'e']\n",
      "['morbid', 'i']\n",
      "['pi', 'u']\n",
      "['sens', 'o']\n",
      "['po', 'i']\n",
      "['bui', 'o']\n",
      "['ma', 'a']\n",
      "['cant', 'a']\n",
      "['grit', 'a']\n",
      "['tell', 'e']\n",
      "['sang', 'o']\n",
      "['film', 'y']\n",
      "['i', 'lies']\n",
      "['o', 'm']\n",
      "['ve', 'o']\n",
      "['ell', 'o']\n",
      "['est', 'o']\n",
      "['ve', 'a']\n",
      "['vers', 'o']\n",
      "['font', 'i']\n",
      "['accord', 'i']\n",
      "['arch', 'i']\n",
      "['have', 'a']\n",
      "['dar', 'i']\n",
      "['a', 'historical']\n",
      "['brahm', 'a']\n",
      "['u', 'k']\n",
      "['vinegar', 'y']\n",
      "['soup', 'e']\n",
      "['wart', 'y']\n",
      "['sage', 'y']\n",
      "['weather', 'y']\n",
      "['ze', 'y']\n",
      "['z', 'a']\n",
      "['som', 'u']\n",
      "['bes', 'o']\n",
      "['e', 'try']\n",
      "['anand', 'a']\n",
      "['psych', 'y']\n",
      "['con', 'i']\n",
      "['pon', 'i']\n",
      "['dour', 'o']\n",
      "['fragrant', 'e']\n",
      "['yan', 'a']\n",
      "['a', 'chivers']\n",
      "['wire', 'y']\n",
      "['diplomat', 'e']\n",
      "['cute', 'y']\n",
      "['enfant', 'a']\n",
      "['trait', 'e']\n",
      "['dampier', 'e']\n",
      "['diet', 'y']\n",
      "['ja', 'a']\n",
      "['rah', 'a']\n",
      "['ha', 'i']\n",
      "['ter', 'e']\n",
      "['gay', 'a']\n",
      "['a', 'a']\n",
      "['zara', 'a']\n",
      "['nah', 'i']\n",
      "['saar', 'e']\n",
      "['wal', 'y']\n",
      "['rag', 'a']\n",
      "['z', 'u']\n",
      "['bom', 'a']\n",
      "['ant', 'o']\n",
      "['ac', 'u']\n",
      "['down', 'a']\n",
      "['winn', 'a']\n",
      "['i', 'er']\n",
      "['noir', 'e']\n",
      "['benoit', 'e']\n",
      "['jet', 'a']\n",
      "['pea', 'u']\n",
      "['malcontent', 'a']\n",
      "['vo', 'u']\n",
      "['e', 'u']\n",
      "['gent', 'e']\n",
      "['vint', 'e']\n",
      "['tel', 'a']\n",
      "['sere', 'i']\n",
      "['mord', 'i']\n",
      "['su', 'a']\n",
      "['fi', 'o']\n",
      "['mei', 'o']\n",
      "['cusp', 'i']\n",
      "['spiral', 'i']\n",
      "['buttock', 'y']\n",
      "['kin', 'e']\n",
      "['rim', 'e']\n",
      "['game', 'y']\n",
      "['accept', 'e']\n",
      "['nebula', 'e']\n",
      "['mer', 'y']\n",
      "['eve', 'y']\n",
      "['protagonist', 'a']\n",
      "['es', 'a']\n",
      "['vist', 'o']\n",
      "['transmit', 'e']\n",
      "['tant', 'a']\n",
      "['merc', 'i']\n",
      "['to', 'i']\n",
      "['tu', 'a']\n",
      "['se', 'i']\n",
      "['elegant', 'e']\n",
      "['sent', 'o']\n",
      "['dall', 'e']\n",
      "['punt', 'e']\n",
      "['pied', 'i']\n",
      "['all', 'e']\n",
      "['spall', 'e']\n",
      "['candid', 'e']\n",
      "['bracci', 'a']\n",
      "['mobil', 'i']\n",
      "['quest', 'i']\n",
      "['vers', 'i']\n",
      "['poet', 'a']\n",
      "['peat', 'y']\n",
      "['march', 'a']\n",
      "['rein', 'o']\n",
      "['mod', 'o']\n",
      "['vien', 'e']\n",
      "['pus', 'o']\n",
      "['nub', 'e']\n",
      "['import', 'a']\n",
      "['brum', 'a']\n",
      "['consist', 'e']\n",
      "['pas', 'a']\n",
      "['sub', 'e']\n",
      "['es', 'e']\n",
      "['a', 'qui']\n",
      "['harp', 'y']\n",
      "['zoo', 'i']\n",
      "['frighten', 'i']\n",
      "['nan', 'e']\n",
      "['aw', 'a']\n",
      "['serr', 'e']\n",
      "['dakin', 'i']\n",
      "['shiver', 'y']\n",
      "['special', 'y']\n",
      "['fu', 'i']\n",
      "['die', 'u']\n",
      "['smit', 'e']\n",
      "['thar', 'u']\n",
      "['dena', 'y']\n",
      "['lini', 'e']\n",
      "['shower', 'y']\n",
      "[\"'s\", 'o']\n",
      "['collin', 'e']\n",
      "['corp', 'o']\n",
      "['yesterday', 'i']\n",
      "['says', 'a']\n",
      "['bem', 'y']\n",
      "['street', 'a']\n",
      "['wasted', 'a']\n",
      "['how', 'i']\n",
      "['one', 'i']\n",
      "['rooms', 'o']\n",
      "['now', 'i']\n",
      "['speed', 'o']\n",
      "['a', 'century']\n",
      "['invents', 'a']\n",
      "['why', 'i']\n",
      "['revere', 'i']\n",
      "['really', 'a']\n",
      "['runners', 'i']\n",
      "['reappear', 'i']\n",
      "['consumed', 'i']\n",
      "['met', 'o']\n",
      "['havas', 'u']\n",
      "['doth', 'e']\n",
      "['i', \"don't\"]\n",
      "['a', 'different']\n",
      "['looming', 'a']\n",
      "['circle', 'a']\n",
      "['meth', 'e']\n",
      "[\"thing's\", 'a']\n",
      "['things', 'i']\n",
      "['when', 'i']\n",
      "['repaired', 'a']\n",
      "['forma', 'y']\n",
      "['forth', 'e']\n",
      "['sound', 'i']\n",
      "['equipment', 'a']\n",
      "['gleam', 'y']\n",
      "['closet', 'o']\n",
      "['shatters', 'a']\n",
      "['time', 'i']\n",
      "['tossed', 'a']\n",
      "['summer', 'a']\n",
      "['remark', 'a']\n",
      "['sometimes', 'i']\n",
      "['made', 'a']\n",
      "['whispered', 'a']\n",
      "['fake', 'y']\n",
      "['poem', 'i']\n",
      "['meant', 'o']\n",
      "['a', 'scratch']\n",
      "['hearth', 'e']\n",
      "['body', 'a']\n",
      "['asking', 'a']\n",
      "['fleece', 'a']\n",
      "['open', 'a']\n",
      "['worm', 'y']\n",
      "['you', 'a']\n",
      "['below', 'a']\n",
      "['recreational', 'a']\n",
      "['okays', 'o']\n",
      "['oklahoma', 'a']\n",
      "['kamal', 'a']\n",
      "['o', 'o']\n",
      "['holds', 'a']\n",
      "['sons', 'i']\n",
      "['believing', 'i']\n",
      "['wass', 'o']\n",
      "['web', 'e']\n",
      "['us', 'i']\n",
      "['motion', 'i']\n",
      "['again', 'i']\n",
      "['brother', 'i']\n",
      "['awesome', 'i']\n",
      "['love', 'i']\n",
      "['quake', 'i']\n",
      "['dreams', 'i']\n",
      "['heaving', 'a']\n",
      "['reading', 'i']\n",
      "['beth', 'e']\n",
      "['disengage', 'a']\n",
      "['prune', 'y']\n",
      "['eyes', 'i']\n",
      "['tip', 'i']\n",
      "['trees', 'a']\n",
      "['speckled', 'y']\n",
      "['hardt', 'o']\n",
      "['flickers', 'a']\n",
      "['jars', 'a']\n",
      "['directions', 'a']\n",
      "['lecher', 'y']\n",
      "['boyt', 'o']\n",
      "['house', 'i']\n",
      "['gazes', 'o']\n",
      "['ends', 'a']\n",
      "['lemon', 'y']\n",
      "['hem', 'i']\n",
      "['five', 'i']\n",
      "['later', 'i']\n",
      "['today', 'a']\n",
      "[\"world's\", 'a']\n",
      "['hors', 'y']\n",
      "['feathers', 'i']\n",
      "['bander', 'a']\n",
      "['i', 'es']\n",
      "['writing', 'a']\n",
      "['nutt', 'i']\n",
      "['leave', 'a']\n",
      "['like', 'a']\n",
      "['through', 'a']\n",
      "['felts', 'o']\n",
      "['easter', 'i']\n",
      "['sort', 'a']\n",
      "['montreal', 'i']\n",
      "['given', 'a']\n",
      "['cinema', 'i']\n",
      "['believe', 'i']\n",
      "['i', 'respond']\n",
      "['sandra', 'i']\n",
      "['a', 'flock']\n",
      "['passed', 'a']\n",
      "['instead', 'a']\n",
      "['i', \"haven't\"]\n",
      "['grab', 'i']\n",
      "['c', 'i']\n",
      "['a', 'butterfly']\n",
      "['way', 'i']\n",
      "['how', 'a']\n",
      "['med', 'o']\n",
      "['yellow', 'y']\n",
      "['o', 'ben']\n",
      "['alt', 'e']\n",
      "['i', 'thought']\n",
      "['flower', 'i']\n",
      "['them', 'a']\n",
      "['underground', 'a']\n",
      "['sad', 'e']\n",
      "['dog', 'o']\n",
      "['makes', 'a']\n",
      "['mar', 'u']\n",
      "['nevertheless', 'i']\n",
      "['i', \"couldn't\"]\n",
      "['lie', 'i']\n",
      "['say', 'i']\n",
      "['tom', 'y']\n",
      "['more', 'i']\n",
      "['back', 'i']\n",
      "['a', 'somewhat']\n",
      "['water', 'i']\n",
      "['cheeks', 'o']\n",
      "['somewhere', 'a']\n",
      "['break', 'a']\n",
      "['window', 'a']\n",
      "['tap', 'a']\n",
      "['casts', 'a']\n",
      "['mississippi', 'i']\n",
      "['death', 'a']\n",
      "['once', 'a']\n",
      "['wayt', 'o']\n",
      "['only', 'a']\n",
      "['shells', 'o']\n",
      "['osh', 'i']\n",
      "['but', 'i']\n",
      "['depth', 'a']\n",
      "['i', 'in']\n",
      "['spend', 'a']\n",
      "['facet', 'o']\n",
      "['doors', 'o']\n",
      "['luster', 'i']\n",
      "['squirrel', 'y']\n",
      "['not', 'a']\n",
      "['between', 'a']\n",
      "['night', 'a']\n",
      "['i', 'live']\n",
      "['freet', 'o']\n",
      "['away', 'i']\n",
      "['by', 'a']\n",
      "['oft', 'o']\n",
      "['nibbles', 'a']\n",
      "['leg', 'a']\n",
      "['with', 'a']\n",
      "['them', 'i']\n",
      "['stephan', 'o']\n",
      "['sun', 'a']\n",
      "['stith', 'y']\n",
      "['beens', 'o']\n",
      "['spurt', 'o']\n",
      "['say', 'a']\n",
      "['i', 'exult']\n",
      "['accident', 'i']\n",
      "['end', 'i']\n",
      "['fern', 'y']\n",
      "['seem', 'y']\n",
      "['breath', 'i']\n",
      "['dead', 'i']\n",
      "['around', 'a']\n",
      "['square', 'a']\n",
      "['dispense', 'a']\n",
      "['praise', 'i']\n",
      "['a', 'wilted']\n",
      "['soar', 'e']\n",
      "['receives', 'a']\n",
      "['hence', 'i']\n",
      "['spot', 'i']\n",
      "['something', 'a']\n",
      "['lying', 'a']\n",
      "['women', 'i']\n",
      "['taking', 'a']\n",
      "['a', 'body']\n",
      "['wish', 'i']\n",
      "['and', 'i']\n",
      "['baby', 'i']\n",
      "['happen', 'i']\n",
      "['head', 'i']\n",
      "['dreaming', 'a']\n",
      "['games', 'i']\n",
      "['comes', 'i']\n",
      "['carry', 'a']\n",
      "['suns', 'o']\n",
      "['way', 'a']\n",
      "['clear', 'o']\n",
      "['you', 'i']\n",
      "['stir', 'i']\n",
      "['before', 'i']\n",
      "['feelings', 'o']\n",
      "['sky', 'a']\n",
      "['sighs', 'a']\n",
      "['morsel', 'i']\n",
      "['wishing', 'a']\n",
      "['vowels', 'a']\n",
      "['knitting', 'a']\n",
      "['rocks', 'i']\n",
      "['shape', 'a']\n",
      "['anything', 'i']\n",
      "['suresh', 'e']\n",
      "['laughs', 'o']\n",
      "['cried', 'a']\n",
      "['youth', 'e']\n",
      "['best', 'i']\n",
      "['toth', 'e']\n",
      "['diligently', 'a']\n",
      "['day', 'i']\n",
      "['cry', 'i']\n",
      "['bendt', 'o']\n",
      "['mass', 'y']\n",
      "['still', 'a']\n",
      "['weaving', 'a']\n",
      "['consults', 'a']\n",
      "['squirts', 'a']\n",
      "['mayben', 'o']\n",
      "['beneath', 'a']\n",
      "['deal', 'a']\n",
      "['dreamed', 'i']\n",
      "['ours', 'a']\n",
      "['smile', 'i']\n",
      "['hight', 'o']\n",
      "['coat', 'a']\n",
      "['ferry', 'a']\n",
      "['pitch', 'y']\n",
      "['become', 'a']\n",
      "['re', 'i']\n",
      "['fire', 'i']\n",
      "['mistakes', 'o']\n",
      "['postcards', 'i']\n",
      "['find', 'a']\n",
      "['into', 'a']\n",
      "['rope', 'y']\n",
      "['o', 'lor']\n",
      "['take', 'a']\n",
      "['satisfaction', 'i']\n",
      "['i', \"can't\"]\n",
      "['mistaken', 'a']\n",
      "['days', 'i']\n",
      "['because', 'i']\n",
      "['then', 'i']\n",
      "['siberia', 'a']\n",
      "['a', 'jo']\n",
      "['wheat', 'y']\n",
      "['i', 'ambs']\n",
      "['a', 'ike']\n",
      "['op', 'a']\n",
      "['u', 'bi']\n",
      "['se', 'y']\n",
      "['manac', 'e']\n",
      "['reform', 'e']\n",
      "['wey', 'e']\n",
      "['e', 'che']\n",
      "['enter', 'e']\n",
      "['yellow', 'e']\n",
      "['yong', 'e']\n",
      "['dew', 'e']\n",
      "['brought', 'e']\n",
      "['around', 'e']\n",
      "['song', 'e']\n",
      "['round', 'e']\n",
      "['door', 'e']\n",
      "['post', 'e']\n",
      "['lai', 'e']\n",
      "['yett', 'e']\n",
      "['mi', 'e']\n",
      "['second', 'e']\n",
      "['unto', 'e']\n",
      "['thi', 'e']\n",
      "['rudd', 'e']\n",
      "['bend', 'e']\n",
      "['y', 'n']\n",
      "['dai', 'e']\n",
      "['wynter', 'e']\n",
      "['rodd', 'e']\n",
      "['death', 'e']\n",
      "['thought', 'e']\n",
      "['cann', 'e']\n",
      "['deft', 'e']\n",
      "['bi', 'e']\n",
      "['owl', 'e']\n",
      "['loud', 'e']\n",
      "['shroud', 'e']\n",
      "['cloud', 'e']\n",
      "['schall', 'e']\n",
      "[\"i'll\", 'e']\n",
      "['acorn', 'e']\n",
      "['copp', 'e']\n",
      "['net', 'e']\n",
      "['feast', 'e']\n",
      "['crowned', 'e']\n",
      "['hott', 'e']\n",
      "['rai', 'e']\n",
      "['pear', 'e']\n",
      "['peed', 'e']\n",
      "['sung', 'e']\n",
      "['most', 'e']\n",
      "['hep', 'e']\n",
      "['woodland', 'e']\n",
      "['drew', 'e']\n",
      "['tempest', 'e']\n",
      "['beneath', 'e']\n",
      "['did', 'e']\n",
      "['led', 'e']\n",
      "['new', 'e']\n",
      "['weed', 'e']\n",
      "['need', 'e']\n",
      "['could', 'e']\n",
      "['a', 'nie']\n",
      "['dead', 'e']\n",
      "['kist', 'e']\n",
      "['which', 'e']\n",
      "['high', 'e']\n",
      "['fall', 'e']\n",
      "['drench', 'e']\n",
      "['list', 'e']\n",
      "['o', 'ere']\n",
      "['abbot', 'e']\n",
      "['convent', 'e']\n",
      "['told', 'e']\n",
      "['might', 'e']\n",
      "['binn', 'e']\n",
      "['sight', 'e']\n",
      "['priest', 'e']\n",
      "['said', 'e']\n",
      "['covent', 'e']\n",
      "['head', 'e']\n",
      "['pouch', 'e']\n",
      "['crouch', 'e']\n",
      "['proud', 'e']\n",
      "['thread', 'e']\n",
      "['groat', 'e']\n",
      "['hallin', 'e']\n",
      "['mai', 'e']\n",
      "['our', 'e']\n",
      "['art', 'e']\n",
      "['reward', 'e']\n",
      "['wai', 'e']\n",
      "['god', 'e']\n",
      "['o', 'd']\n",
      "['lux', 'e']\n",
      "['gean', 'e']\n",
      "['arab', 'i']\n",
      "['graham', 'e']\n",
      "['sweet', 'y']\n",
      "['cabal', 'a']\n",
      "[\"'n\", 'o']\n",
      "['amid', 'a']\n",
      "['werk', 'i']\n",
      "['sor', 'a']\n",
      "['koss', 'e']\n",
      "['c', 'u']\n",
      "['buck', 'e']\n",
      "['muri', 'e']\n",
      "['th', 'u']\n",
      "['york', 'a']\n",
      "['taint', 'e']\n",
      "['ken', 'e']\n",
      "['f', 'o']\n",
      "['thul', 'e']\n",
      "['world', 'e']\n",
      "['tour', 'e']\n",
      "['sound', 'e']\n",
      "['a', 'ince']\n",
      "['ho', 'a']\n",
      "['cool', 'y']\n",
      "['a', 'swirl']\n",
      "['spider', 'y']\n",
      "['madron', 'e']\n",
      "['punk', 'a']\n",
      "['herk', 'y']\n",
      "['sette', 'e']\n",
      "['wife', 'y']\n",
      "['sent', 'e']\n",
      "['chery', 'e']\n",
      "['rind', 'e']\n",
      "['without', 'e']\n",
      "['hadd', 'e']\n",
      "['e', 'y']\n",
      "['y', 'r']\n",
      "['chuck', 'y']\n",
      "['der', 'e']\n",
      "['fader', 'e']\n",
      "['moder', 'e']\n",
      "['son', 'e']\n",
      "['papp', 'e']\n",
      "['speir', 'e']\n",
      "['tender', 'e']\n",
      "['kever', 'e']\n",
      "['ruth', 'e']\n",
      "['hert', 'e']\n",
      "['ill', 'a']\n",
      "['past', 'y']\n",
      "['crap', 'e']\n",
      "['shill', 'y']\n",
      "['shall', 'y']\n",
      "['mary', 'e']\n",
      "['bust', 'e']\n",
      "['real', 'y']\n",
      "['eber', 'y']\n",
      "['mock', 'e']\n",
      "['vein', 'y']\n",
      "['rape', 'y']\n",
      "['tress', 'y']\n",
      "['lucent', 'o']\n",
      "['golf', 'o']\n",
      "['lok', 'i']\n",
      "['ours', 'i']\n",
      "['asked', 'e']\n",
      "['spend', 'e']\n",
      "['as', 'e']\n",
      "['lett', 'e']\n",
      "['e', 'quite']\n",
      "['right', 'e']\n",
      "['amaranth', 'a']\n",
      "['crop', 'e']\n",
      "['arrow', 'y']\n",
      "['sopp', 'y']\n",
      "[\"i'm\", 'e']\n",
      "['mag', 'e']\n",
      "['rah', 'u']\n",
      "['ip', 'a']\n",
      "['crout', 'e']\n",
      "['bosom', 'y']\n",
      "['prevail', 'e']\n",
      "['joh', 'a']\n",
      "['warr', 'a']\n",
      "['pastoral', 'e']\n",
      "['marquis', 'e']\n",
      "['hybl', 'a']\n",
      "['nicki', 'e']\n",
      "['morph', 'o']\n",
      "['a', 'vengeful']\n",
      "['scragg', 'y']\n",
      "['reti', 'e']\n",
      "['e', 'ying']\n",
      "['ib', 'o']\n",
      "['asphalt', 'e']\n",
      "['oh', 'o']\n",
      "['rasch', 'i']\n",
      "['fidget', 'y']\n",
      "['harmless', 'e']\n",
      "['e', 'um']\n",
      "['radiant', 'e']\n",
      "['sund', 'a']\n",
      "['naim', 'a']\n",
      "['villain', 'y']\n",
      "['compost', 'y']\n",
      "['ruf', 'a']\n",
      "['bloom', 'y']\n",
      "['villari', 'o']\n",
      "['verri', 'o']\n",
      "['y', 'lang']\n",
      "['mist', 'a']\n",
      "['glue', 'y']\n",
      "['wife', 'i']\n",
      "['branch', 'y']\n",
      "['cuban', 'o']\n",
      "['syrup', 'y']\n",
      "['dom', 'o']\n",
      "['wheel', 'e']\n",
      "['word', 'e']\n",
      "['neat', 'e']\n",
      "['spool', 'e']\n",
      "['reel', 'e']\n",
      "['loom', 'e']\n",
      "['d', 'y']\n",
      "['loaf', 'e']\n",
      "['wheat', 'e']\n",
      "['bespeak', 'e']\n",
      "['eat', 'e']\n",
      "['mims', 'y']\n",
      "['mom', 'e']\n",
      "['a', 'flower']\n",
      "['ling', 'e']\n",
      "['hell', 'y']\n",
      "['dana', 'e']\n",
      "['roth', 'a']\n",
      "['munn', 'y']\n",
      "['barn', 'e']\n",
      "['bean', 'y']\n",
      "['brin', 'y']\n",
      "['loft', 'e']\n",
      "['mow', 'e']\n",
      "['a', 'dazzle']\n",
      "['que', 'e']\n",
      "['sheen', 'y']\n",
      "['a', 'night']\n",
      "['vast', 'y']\n",
      "['billow', 'y']\n",
      "['comp', 'a']\n",
      "['n', 'y']\n",
      "['nor', 'i']\n",
      "['initiator', 'y']\n",
      "['maser', 'u']\n",
      "['wis', 'a']\n",
      "[\"'s\", 'a']\n",
      "['flagg', 'y']\n",
      "['y', 'minted']\n",
      "['vim', 'y']\n",
      "['drown', 'e']\n",
      "['x', 'y']\n",
      "['wound', 'e']\n",
      "['confound', 'e']\n",
      "['found', 'e']\n",
      "['lunar', 'y']\n",
      "['liu', 'e']\n",
      "['e', 'cart']\n",
      "['titian', 'a']\n",
      "['skipp', 'e']\n",
      "['whipp', 'e']\n",
      "['law', 'e']\n",
      "['inn', 'e']\n",
      "['o', 'on']\n",
      "['defend', 'e']\n",
      "['benign', 'e']\n",
      "['shosh', 'o']\n",
      "['sword', 'y']\n",
      "['myrrh', 'a']\n",
      "['piecemeal', 'e']\n",
      "[\"'s\", 'e']\n",
      "['sod', 'y']\n",
      "['cider', 'y']\n",
      "['faus', 'e']\n",
      "['lark', 'y']\n",
      "['nowa', 'y']\n",
      "['attest', 'e']\n",
      "['o', 'vis']\n",
      "['oh', 'e']\n",
      "['tener', 'i']\n",
      "['qua', 'e']\n",
      "['spire', 'a']\n",
      "['polk', 'e']\n",
      "['sultan', 'a']\n",
      "['jeer', 'a']\n",
      "['kutch', 'a']\n",
      "['puck', 'a']\n",
      "['tins', 'y']\n",
      "['flake', 'y']\n",
      "['josef', 'a']\n",
      "['mo', 'a']\n",
      "['mr', 'i']\n",
      "['glob', 'i']\n",
      "['redact', 'i']\n",
      "['i', 'conic']\n",
      "['nan', 'i']\n",
      "['dann', 'u']\n",
      "['dug', 'e']\n",
      "['egg', 'y']\n",
      "['r', 'y']\n",
      "['i', 'e']\n",
      "['colored', 'y']\n",
      "['moir', 'e']\n",
      "['spring', 'e']\n",
      "['longing', 'e']\n",
      "['thing', 'e']\n",
      "['bliss', 'e']\n",
      "['bring', 'e']\n",
      "['y', 'noh']\n",
      "['broun', 'e']\n",
      "['y', 'make']\n",
      "['fey', 'e']\n",
      "['forth', 'i']\n",
      "['y', 'lent']\n",
      "['en', 'y']\n",
      "['rev', 'e']\n",
      "['northern', 'e']\n",
      "['burd', 'e']\n",
      "['bour', 'e']\n",
      "['fond', 'e']\n",
      "['yet', 'e']\n",
      "['fong', 'e']\n",
      "['he', 'o']\n",
      "['brem', 'e']\n",
      "['rest', 'e']\n",
      "['led', 'y']\n",
      "['gracious', 'e']\n",
      "['lili', 'e']\n",
      "['wax', 'e']\n",
      "['u', 'ard']\n",
      "['fold', 'u']\n",
      "['i', 've']\n",
      "['lad', 'i']\n",
      "['wept', 'e']\n",
      "['travail', 'e']\n",
      "['the', 'i']\n",
      "['befall', 'e']\n",
      "['spill', 'e']\n",
      "['behind', 'e']\n",
      "['worth', 'i']\n",
      "['lech', 'e']\n",
      "['hier', 'e']\n",
      "['no', 'u']\n",
      "['about', 'e']\n",
      "['wand', 'e']\n",
      "['stand', 'e']\n",
      "['great', 'e']\n",
      "['dyed', 'e']\n",
      "['reynold', 'e']\n",
      "['yer', 'e']\n",
      "['get', 'e']\n",
      "['sped', 'e']\n",
      "['fasting', 'e']\n",
      "['stuard', 'e']\n",
      "['et', 'e']\n",
      "['avow', 'e']\n",
      "['such', 'e']\n",
      "['drink', 'e']\n",
      "['bold', 'e']\n",
      "['shrewd', 'e']\n",
      "['hynd', 'e']\n",
      "['an', 'i']\n",
      "['ask', 'e']\n",
      "['sword', 'e']\n",
      "['hand', 'e']\n",
      "['harm', 'e']\n",
      "['shot', 'e']\n",
      "['gren', 'e']\n",
      "['clothing', 'e']\n",
      "['merk', 'e']\n",
      "['pound', 'e']\n",
      "['crist', 'e']\n",
      "['herd', 'e']\n",
      "['whan', 'e']\n",
      "['worth', 'e']\n",
      "['tech', 'e']\n",
      "['outlaw', 'e']\n",
      "['hath', 'e']\n",
      "['sworn', 'e']\n",
      "['remedy', 'e']\n",
      "['tham', 'e']\n",
      "['y', 'bought']\n",
      "['trow', 'e']\n",
      "['fantasy', 'e']\n",
      "['flour', 'e']\n",
      "['study', 'e']\n",
      "['likeness', 'e']\n",
      "['sel', 'y']\n",
      "['a', 'maille']\n",
      "['party', 'e']\n",
      "['fort', 'o']\n",
      "['sen', 'e']\n",
      "['a', 'port']\n",
      "['a', 'vance']\n",
      "['servis', 'e']\n",
      "['cort', 'a']\n",
      "['destin', 'o']\n",
      "['civil', 'i']\n",
      "['mata', 'o']\n",
      "['vibrator', 'y']\n",
      "['a', 'k']\n",
      "['y', 'f']\n",
      "['aloud', 'e']\n",
      "['haunt', 'e']\n",
      "['corrupt', 'e']\n",
      "['folk', 'e']\n",
      "['derk', 'e']\n",
      "['should', 'a']\n",
      "['habit', 'e']\n",
      "['malvern', 'e']\n",
      "['men', 'e']\n",
      "['e', 'oden']\n",
      "['ever', 'e']\n",
      "['jam', 'e']\n",
      "['be', 'o']\n",
      "['four', 'e']\n",
      "['mony', 'e']\n",
      "['money', 'e']\n",
      "['few', 'e']\n",
      "['mold', 'e']\n",
      "['schuld', 'e']\n",
      "['simony', 'e']\n",
      "['defy', 'e']\n",
      "['es', 'o']\n",
      "['vert', 'u']\n",
      "['obey', 'e']\n",
      "['putt', 'e']\n",
      "['duress', 'e']\n",
      "['virgil', 'e']\n",
      "['e', 'gal']\n",
      "['y', 'le']\n",
      "['trust', 'e']\n",
      "['hurd', 'y']\n",
      "['a', 'vernal']\n",
      "['steven', 'e']\n",
      "['wed', 'e']\n",
      "['ply', 'e']\n",
      "['idol', 'e']\n",
      "['cac', 'e']\n",
      "['form', 'e']\n",
      "['yon', 'e']\n",
      "['buri', 'e']\n",
      "['a', 'aliyah']\n",
      "['lows', 'y']\n",
      "['bows', 'y']\n",
      "['lewd', 'e']\n",
      "['understand', 'e']\n",
      "['themself', 'e']\n",
      "['monn', 'y']\n",
      "['honn', 'y']\n",
      "['loth', 'e']\n",
      "['syd', 'e']\n",
      "['malt', 'e']\n",
      "['whet', 'e']\n",
      "['peck', 'e']\n",
      "['flax', 'e']\n",
      "['wash', 'e']\n",
      "['many', 'e']\n",
      "['hard', 'e']\n",
      "['never', 'e']\n",
      "['thoro', 'u']\n",
      "['nether', 'e']\n",
      "['i', 'cast']\n",
      "['strength', 'e']\n",
      "['kett', 'y']\n",
      "['bully', 'a']\n",
      "['pack', 'i']\n",
      "['anse', 'o']\n",
      "['hans', 'e']\n",
      "['aaa', 'a']\n",
      "['i', 'sidor']\n",
      "['hatt', 'o']\n",
      "['frogg', 'y']\n",
      "['art', 'i']\n",
      "['e', 'a']\n",
      "['y', 'israel']\n",
      "['an', 'u']\n",
      "['ph', 'o']\n",
      "['slush', 'y']\n",
      "['cheer', 'o']\n",
      "['brill', 'o']\n",
      "['nigg', 'a']\n",
      "['massad', 'e']\n",
      "['dore', 'e']\n",
      "['slather', 'y']\n",
      "['mam', 'i']\n",
      "['benda', 'y']\n",
      "['a', 'nuncio']\n",
      "['ryon', 'e']\n",
      "['p', 'e']\n",
      "['err', 'e']\n",
      "['zeal', 'e']\n",
      "['muse', 'o']\n",
      "['u', 'topic']\n",
      "['gang', 'a']\n",
      "['ducal', 'e']\n",
      "['pou', 'i']\n",
      "['shake', 'y']\n",
      "['as', 'i']\n",
      "['tusk', 'y']\n",
      "['im', 'e']\n",
      "['o', 'rotund']\n",
      "['di', 'o']\n",
      "['i', 'rides']\n",
      "['conjunctiva', 'e']\n",
      "['sansom', 'e']\n",
      "['progress', 'o']\n",
      "['watt', 'a']\n",
      "['dapp', 'a']\n",
      "['yeast', 'y']\n",
      "['metropol', 'e']\n",
      "['quietly', 'e']\n",
      "['vic', 'i']\n",
      "['sharon', 'a']\n",
      "['o', 'cd']\n",
      "['dag', 'o']\n",
      "['tass', 'e']\n",
      "['ira', 'e']\n",
      "['moon', 'y']\n",
      "['music', 'o']\n",
      "['pom', 'e']\n",
      "['joke', 'y']\n",
      "['vault', 'y']\n",
      "['e', 'ping']\n",
      "['kumar', 'a']\n",
      "['nu', 'i']\n",
      "['kidd', 'o']\n",
      "['envy', 'e']\n",
      "['spurn', 'e']\n",
      "['daunt', 'e']\n",
      "['lok', 'e']\n",
      "['a', 'sparkle']\n",
      "['o', 'ink']\n",
      "['lorin', 'e']\n",
      "['kant', 'o']\n",
      "['sasha', 'y']\n",
      "['expand', 'o']\n",
      "['witcher', 'y']\n",
      "['ay', 'a']\n",
      "['a', 'tremble']\n",
      "['i', 'c']\n",
      "['reveal', 'e']\n",
      "['tar', 'y']\n",
      "['cloth', 'o']\n",
      "['heng', 'e']\n",
      "['ribbon', 'y']\n",
      "['paddock', 'y']\n",
      "['trahern', 'e']\n",
      "['rook', 'y']\n",
      "['solid', 'i']\n",
      "['brom', 'e']\n",
      "['u', 'bree']\n",
      "['dev', 'i']\n",
      "['rink', 'y']\n",
      "['i', 'zed']\n",
      "['stag', 'y']\n",
      "['polaris', 'e']\n",
      "['polycast', 'e']\n",
      "['so', 'i']\n",
      "['broth', 'a']\n",
      "['flame', 'y']\n",
      "['pavlov', 'a']\n",
      "['malle', 'e']\n",
      "['lint', 'y']\n",
      "['hung', 'a']\n",
      "['rum', 'i']\n",
      "['hiss', 'y']\n",
      "['lum', 'e']\n",
      "['spent', 'o']\n",
      "['fust', 'y']\n",
      "['vin', 'o']\n",
      "['deb', 'e']\n",
      "['disgust', 'o']\n",
      "['trod', 'e']\n",
      "['tis', 'a']\n",
      "['o', 'oooh']\n",
      "['a', 'voir']\n",
      "['gutt', 'a']\n",
      "['perch', 'a']\n",
      "['a', 'flicker']\n",
      "['ka', 'e']\n",
      "['sh', 'o']\n",
      "['i', 'we']\n",
      "['o', 'bi']\n",
      "['ts', 'i']\n",
      "['pik', 'i']\n",
      "['glow', 'y']\n",
      "['torti', 'e']\n",
      "['bliss', 'y']\n",
      "['a', 'tilt']\n",
      "['tab', 'e']\n",
      "['conch', 'o']\n",
      "['u', 'te']\n",
      "['peyot', 'e']\n",
      "['wallow', 'a']\n",
      "['sit', 'u']\n",
      "['silt', 'y']\n",
      "['gilt', 'y']\n",
      "['maund', 'y']\n",
      "['quant', 'a']\n",
      "['off', 'a']\n",
      "['saal', 'e']\n",
      "['church', 'y']\n",
      "['kia', 'i']\n",
      "['plum', 'y']\n",
      "['gung', 'a']\n",
      "['henn', 'y']\n",
      "['serb', 'o']\n",
      "['hors', 'a']\n",
      "['y', 'all']\n",
      "['thread', 'y']\n",
      "['shirt', 'y']\n",
      "['je', 'u']\n",
      "['minh', 'a']\n",
      "['berg', 'y']\n",
      "['u', 'b']\n",
      "['no', 'o']\n",
      "['motl', 'y']\n",
      "['suk', 'y']\n",
      "['cooker', 'y']\n",
      "['sweep', 'y']\n",
      "['success', 'e']\n",
      "['roar', 'e']\n",
      "['known', 'e']\n",
      "['o', 'v']\n",
      "['trent', 'a']\n",
      "['trent', 'e']\n",
      "['squall', 'y']\n",
      "['wants', 'a']\n",
      "['faker', 'y']\n",
      "['think', 'y']\n",
      "['punt', 'i']\n",
      "['condenser', 'y']\n",
      "['carn', 'i']\n",
      "['leopard', 'y']\n",
      "['walsh', 'e']\n",
      "['colm', 'a']\n",
      "['ori', 'a']\n",
      "['rooker', 'y']\n",
      "['kam', 'e']\n",
      "['darling', 'e']\n",
      "['pretti', 'e']\n",
      "['eis', 'e']\n",
      "['per', 'i']\n",
      "['unkind', 'e']\n",
      "['mistress', 'e']\n",
      "['jean', 'y']\n",
      "['heart', 'e']\n",
      "['myself', 'e']\n",
      "['cul', 'e']\n",
      "['broom', 'y']\n",
      "['know', 'e']\n",
      "['darke', 'y']\n",
      "['conceit', 'y']\n",
      "['amiss', 'e']\n",
      "['tay', 'o']\n",
      "['sulphur', 'y']\n",
      "['matador', 'e']\n",
      "['croup', 'e']\n",
      "['dwarf', 'y']\n",
      "['bok', 'e']\n",
      "['corridor', 'e']\n",
      "['y', 'gazed']\n",
      "['ruin', 'i']\n",
      "['serb', 'a']\n",
      "['fast', 'i']\n",
      "['pomp', 'e']\n",
      "['erb', 'a']\n",
      "['a', 'dallas']\n",
      "['du', 'a']\n",
      "['g', 'i']\n",
      "['put', 'a']\n",
      "['r', 'i']\n",
      "['vet', 'i']\n",
      "['u', 'tara']\n",
      "['et', 'i']\n",
      "['bir', 'e']\n",
      "['gu', 'i']\n",
      "['u', 'di']\n",
      "['hoch', 'e']\n",
      "['lombardi', 'a']\n",
      "['brent', 'a']\n",
      "['lady', 'e']\n",
      "['fe', 'o']\n",
      "['sort', 'e']\n",
      "['don', 'o']\n",
      "['front', 'e']\n",
      "['gallic', 'i']\n",
      "['arment', 'i']\n",
      "['bracci', 'o']\n",
      "['gent', 'i']\n",
      "['vint', 'a']\n",
      "['e', 'que']\n",
      "['tort', 'o']\n",
      "['cels', 'a']\n",
      "['fer', 'i']\n",
      "['tol', 'a']\n",
      "['tar', 'e']\n",
      "['augur', 'y']\n",
      "['pens', 'a']\n",
      "['tac', 'i']\n",
      "['null', 'a']\n",
      "['ad', 'i']\n",
      "['poch', 'o']\n",
      "['pu', 'o']\n",
      "['no', 'i']\n",
      "['molt', 'o']\n",
      "['errant', 'i']\n",
      "['arm', 'e']\n",
      "['b', 'u']\n",
      "['mess', 'i']\n",
      "['uber', 'a']\n",
      "['finger', 'e']\n",
      "['quant', 'o']\n",
      "['volt', 'e']\n",
      "['ro', 'o']\n",
      "['a', 'basement']\n",
      "['a', 'frits']\n",
      "['tener', 'o']\n",
      "['nymph', 'a']\n",
      "['crimp', 'y']\n",
      "['wad', 'y']\n",
      "['divi', 'e']\n",
      "['nurse', 'y']\n",
      "['bubb', 'y']\n",
      "['rol', 'y']\n",
      "['sulk', 'y']\n",
      "['jing', 'o']\n",
      "['sen', 'o']\n",
      "['ardor', 'e']\n",
      "['cavalier', 'o']\n",
      "['basel', 'y']\n",
      "['son', 'o']\n",
      "['ham', 'o']\n",
      "['gascon', 'y']\n",
      "['macon', 'e']\n",
      "['paladin', 'i']\n",
      "['pagani', 'a']\n",
      "['desert', 'o']\n",
      "['furor', 'e']\n",
      "['verb', 'o']\n",
      "['antic', 'a']\n",
      "['a', 'bisso']\n",
      "['diss', 'e']\n",
      "['sorell', 'a']\n",
      "['lament', 'a']\n",
      "['feb', 'o']\n",
      "['su', 'o']\n",
      "['vara', 'i']\n",
      "['pros', 'a']\n",
      "['quant', 'i']\n",
      "['post', 'o']\n",
      "['stat', 'a']\n",
      "['quell', 'a']\n",
      "['fi', 'a']\n",
      "['divis', 'o']\n",
      "['quest', 'o']\n",
      "['venn', 'i']\n",
      "['kind', 'e']\n",
      "['signor', 'y']\n",
      "['bertucci', 'o']\n",
      "['calendar', 'o']\n",
      "['blond', 'o']\n",
      "['lion', 'i']\n",
      "['miser', 'i']\n",
      "['ted', 'e']\n",
      "['rende', 'o']\n",
      "['o', 've']\n",
      "['corr', 'o']\n",
      "['batt', 'o']\n",
      "['bale', 'a']\n",
      "['armour', 'y']\n",
      "['serr', 'y']\n",
      "['a', 'base']\n",
      "['off', 'i']\n",
      "['toll', 'i']\n",
      "['recent', 'i']\n",
      "['lombard', 'y']\n",
      "['cane', 'a']\n",
      "['tant', 'i']\n",
      "['intention', 'e']\n",
      "['e', 'gad']\n",
      "['grande', 'e']\n",
      "['wolff', 'e']\n",
      "['regard', 'e']\n",
      "['arab', 'y']\n",
      "['holl', 'a']\n",
      "['chin', 'e']\n",
      "['o', 'limp']\n",
      "['hoon', 'i']\n",
      "['lico', 'o']\n",
      "['sparr', 'y']\n",
      "['carr', 'i']\n",
      "['pat', 'i']\n",
      "['haber', 'e']\n",
      "['fel', 'o']\n",
      "['gall', 'y']\n",
      "['cobb', 'y']\n",
      "['sul', 'i']\n",
      "['mir', 'y']\n",
      "['peon', 'a']\n",
      "['character', 'y']\n",
      "['a', 'flow']\n",
      "['luft', 'y']\n",
      "['tocco', 'a']\n",
      "['stag', 'u']\n",
      "['drought', 'y']\n",
      "['lawn', 'y']\n",
      "['a', 'fret']\n",
      "['e', 'radiant']\n",
      "['e', 'norm']\n",
      "['mish', 'e']\n",
      "['iago', 'o']\n",
      "['g', 'e']\n",
      "['nahm', 'a']\n",
      "['pa', 'u']\n",
      "['onawa', 'y']\n",
      "['mitch', 'e']\n",
      "['koh', 'o']\n",
      "['shad', 'a']\n",
      "['whats', 'o']\n",
      "['smith', 'y']\n",
      "['chin', 'y']\n",
      "['fund', 'y']\n",
      "['ale', 'e']\n",
      "['y', 'aller']\n",
      "['prem', 'i']\n",
      "['succor', 'y']\n",
      "['wind', 'o']\n",
      "['mab', 'y']\n",
      "['i', 'ze']\n",
      "['a', 'chin']\n",
      "['fe', 'a']\n",
      "['a', 'z']\n",
      "['riz', 'e']\n",
      "['e', 'ph']\n",
      "['sapp', 'y']\n",
      "['a', 'nigh']\n",
      "['pu', 'y']\n",
      "['aim', 'a']\n",
      "['tan', 'e']\n",
      "['a', 'merced']\n",
      "['rad', 'e']\n",
      "['crowd', 'y']\n",
      "['calm', 'y']\n",
      "['jock', 'y']\n",
      "['tit', 'e']\n",
      "['strong', 'e']\n",
      "['sill', 'e']\n",
      "['arthur', 'e']\n",
      "['men', 'y']\n",
      "['y', 'wain']\n",
      "['y', 'wan']\n",
      "['ich', 'e']\n",
      "['sturn', 'e']\n",
      "['ferd', 'e']\n",
      "['bobb', 'e']\n",
      "['scher', 'e']\n",
      "['leu', 'e']\n",
      "['wond', 'e']\n",
      "['wyss', 'e']\n",
      "['won', 'e']\n",
      "['prayer', 'e']\n",
      "['amend', 'e']\n",
      "['fer', 'e']\n",
      "['torn', 'e']\n",
      "['slept', 'e']\n",
      "['spell', 'e']\n",
      "['chek', 'e']\n",
      "['let', 'e']\n",
      "['nobel', 'e']\n",
      "['bout', 'e']\n",
      "['hau', 'e']\n",
      "['mess', 'e']\n",
      "['out', 'e']\n",
      "['fell', 'e']\n",
      "['aloft', 'e']\n",
      "['lot', 'e']\n",
      "['brun', 'y']\n",
      "['hatt', 'e']\n",
      "['gress', 'e']\n",
      "['del', 'e']\n",
      "['glod', 'e']\n",
      "['cord', 'e']\n",
      "['wen', 'e']\n",
      "['acord', 'e']\n",
      "['nay', 'e']\n",
      "['sib', 'i']\n",
      "['vein', 'e']\n",
      "['bosom', 'e']\n",
      "['e', 'leg']\n",
      "['cul', 'o']\n",
      "['viper', 'a']\n",
      "['mens', 'e']\n",
      "['yow', 'e']\n",
      "['ers', 'e']\n",
      "['o', 'nie']\n",
      "['meer', 'e']\n",
      "['hamel', 'y']\n",
      "['bur', 'e']\n",
      "['hoyt', 'e']\n",
      "['dour', 'e']\n",
      "['o', 'urie']\n",
      "['randi', 'e']\n",
      "['orr', 'a']\n",
      "['gaus', 'y']\n",
      "['gig', 'a']\n",
      "['roost', 'y']\n",
      "['violin', 'o']\n",
      "['antonin', 'e']\n",
      "['mans', 'e']\n",
      "['todd', 'y']\n",
      "['cow', 'e']\n",
      "['irwin', 'e']\n",
      "['coil', 'a']\n",
      "['mo', 'u']\n",
      "['spa', 'e']\n",
      "['sherr', 'a']\n",
      "['gran', 'e']\n",
      "['que', 'y']\n",
      "['k', 'y']\n",
      "['lows', 'e']\n",
      "['dort', 'y']\n",
      "['gulli', 'e']\n",
      "['deposit', 'e']\n",
      "['lent', 'e']\n",
      "['bleak', 'y']\n",
      "['cliff', 'y']\n",
      "['epoch', 'a']\n",
      "['pal', 'y']\n",
      "['develop', 'e']\n",
      "['jam', 'y']\n",
      "['hal', 'y']\n",
      "['brier', 'y']\n",
      "['nanni', 'e']\n",
      "['heath', 'y']\n",
      "['hazell', 'y']\n",
      "['gowan', 'y']\n",
      "['awe', 'e']\n",
      "['lippi', 'e']\n",
      "['tait', 'i']\n",
      "['gent', 'y']\n",
      "['branki', 'e']\n",
      "['fell', 'y']\n",
      "['daren', 'a']\n",
      "['sun', 'e']\n",
      "['duns', 'e']\n",
      "['bro', 'o']\n",
      "['baum', 'y']\n",
      "['carli', 'e']\n",
      "['herr', 'y']\n",
      "['tryst', 'e']\n",
      "['fier', 'e']\n",
      "['rebut', 'e']\n",
      "['scot', 'a']\n",
      "['te', 'y']\n",
      "['wars', 'e']\n",
      "['a', 'lope']\n",
      "['minh', 'o']\n",
      "['i', 'socrates']\n",
      "['german', 'a']\n",
      "['di', 'u']\n",
      "['bud', 'a']\n",
      "['tartar', 'y']\n",
      "['minet', 'y']\n",
      "['file', 'y']\n",
      "['bud', 'e']\n",
      "['duck', 'y']\n",
      "['brown', 'y']\n",
      "['twitch', 'y']\n",
      "['kitch', 'y']\n",
      "['chick', 'y']\n",
      "['mitch', 'y']\n",
      "['shovel', 'y']\n",
      "['fizz', 'y']\n",
      "['blatter', 'y']\n",
      "['humm', 'y']\n",
      "['tak', 'y']\n",
      "['bak', 'y']\n",
      "['mak', 'y']\n",
      "['noll', 'y']\n",
      "['eel', 'y']\n",
      "['peel', 'y']\n",
      "['twirl', 'y']\n",
      "['dish', 'y']\n",
      "['goos', 'y']\n",
      "['moos', 'y']\n",
      "['boose', 'y']\n",
      "['woos', 'y']\n",
      "['eggs', 'y']\n",
      "['black', 'y']\n",
      "['mink', 'y']\n",
      "['kit', 'y']\n",
      "['whit', 'y']\n",
      "['sight', 'y']\n",
      "['hark', 'y']\n",
      "['park', 'y']\n",
      "['sous', 'y']\n",
      "['owl', 'y']\n",
      "['prowl', 'y']\n",
      "['howl', 'y']\n",
      "['fowl', 'y']\n",
      "['pump', 'y']\n",
      "['slump', 'y']\n",
      "['thump', 'y']\n",
      "['quail', 'y']\n",
      "['fail', 'y']\n",
      "['stump', 'y']\n",
      "['tail', 'y']\n",
      "['shrimp', 'y']\n",
      "['thrush', 'y']\n",
      "['hush', 'y']\n",
      "['flush', 'y']\n",
      "['urn', 'y']\n",
      "['burn', 'y']\n",
      "['turn', 'y']\n",
      "['vin', 'y']\n",
      "['win', 'y']\n",
      "['twin', 'y']\n",
      "['crud', 'y']\n",
      "['wink', 'y']\n",
      "['blink', 'y']\n",
      "['bong', 'y']\n",
      "['tinkled', 'y']\n",
      "['tut', 'e']\n",
      "['a', 'ac']\n",
      "['e', 'book']\n",
      "['calla', 'y']\n",
      "['visa', 'e']\n",
      "['meat', 'u']\n",
      "['gnat', 'e']\n",
      "['dens', 'a']\n",
      "['whang', 'o']\n",
      "['whisker', 'y']\n",
      "['mola', 'y']\n",
      "['hockett', 'y']\n",
      "['pill', 'y']\n",
      "['a', 'po']\n",
      "['nor', 'e']\n",
      "['dodd', 'y']\n",
      "['mena', 'i']\n",
      "['tropp', 'o']\n",
      "['bela', 'y']\n",
      "['marlin', 'e']\n",
      "['prop', 'e']\n",
      "['bib', 'e']\n",
      "['kicker', 'e']\n",
      "['rapid', 'e']\n",
      "['e', 'i']\n",
      "['pucker', 'y']\n",
      "['alls', 'o']\n",
      "['purt', 'y']\n",
      "['fide', 'i']\n",
      "['ai', 'e']\n",
      "['personal', 'e']\n",
      "['wo', 'a']\n",
      "['ru', 'y']\n",
      "['dorm', 'y']\n",
      "['gripp', 'e']\n",
      "['bille', 'e']\n",
      "['e', 'books']\n",
      "['e', 'text']\n",
      "['e', \"book's\"]\n",
      "['o', 'cram']\n",
      "['tuft', 'y']\n",
      "['straw', 'y']\n",
      "['night', 'y']\n",
      "['a', 'dry']\n",
      "['sting', 'o']\n",
      "['wo', 'i']\n",
      "['silo', 'a']\n",
      "['trott', 'y']\n",
      "['sty', 'e']\n",
      "['hesse', 'y']\n",
      "['trew', 'e']\n",
      "['pitt', 'y']\n",
      "['glori', 'e']\n",
      "['dross', 'e']\n",
      "['ide', 'e']\n",
      "['fork', 'y']\n",
      "['weakness', 'e']\n",
      "['floor', 'e']\n",
      "['ned', 'e']\n",
      "['among', 'e']\n",
      "['length', 'e']\n",
      "['faith', 'e']\n",
      "['very', 'e']\n",
      "['knock', 'e']\n",
      "['attend', 'e']\n",
      "['send', 'e']\n",
      "['stead', 'e']\n",
      "['raging', 'e']\n",
      "['guiltless', 'e']\n",
      "['rights', 'o']\n",
      "['confess', 'e']\n",
      "['cress', 'e']\n",
      "['resign', 'e']\n",
      "['monitor', 'y']\n",
      "['scalp', 'e']\n",
      "['seldom', 'e']\n",
      "['custom', 'e']\n",
      "['contempt', 'u']\n",
      "['quest', 'a']\n",
      "['u', 'mile']\n",
      "['pur', 'o']\n",
      "['vat', 'i']\n",
      "['naus', 'i']\n",
      "['o', 'don']\n",
      "['athanas', 'e']\n",
      "['aer', 'y']\n",
      "['savell', 'a']\n",
      "['pros', 'y']\n",
      "['flea', 'y']\n",
      "['greg', 'e']\n",
      "['a', 'ethiopia']\n",
      "['jon', 'a']\n",
      "['pot', 'i']\n",
      "['omni', 'a']\n",
      "['arch', 'y']\n",
      "['a', 'ethereal']\n",
      "['si', 'a']\n",
      "['baba', 'i']\n",
      "['papa', 'i']\n",
      "['qui', 'a']\n",
      "['instruct', 'a']\n",
      "['tener', 'e']\n",
      "['edit', 'a']\n",
      "['contender', 'e']\n",
      "['nit', 'i']\n",
      "['excite', 'e']\n",
      "['place', 'e']\n",
      "['manier', 'e']\n",
      "['pense', 'e']\n",
      "['a', 'gens']\n",
      "['important', 'e']\n",
      "['e', 'gorge']\n",
      "['conform', 'e']\n",
      "['evident', 'e']\n",
      "['system', 'e']\n",
      "['humana', 'e']\n",
      "['fact', 'a']\n",
      "['vis', 'o']\n",
      "['part', 'u']\n",
      "['paschal', 'e']\n",
      "['fraud', 'e']\n",
      "['leth', 'i']\n",
      "['debauche', 'e']\n",
      "['auto', 'i']\n",
      "['e', 'it']\n",
      "['est', 'i']\n",
      "['out', 'o']\n",
      "['e', 'pi']\n",
      "['kat', 'a']\n",
      "['sin', 'u']\n",
      "['hospital', 'i']\n",
      "['ill', 'i']\n",
      "['aura', 'e']\n",
      "['culpa', 'e']\n",
      "['ugolin', 'o']\n",
      "['tutt', 'o']\n",
      "['divers', 'o']\n",
      "['en', 'i']\n",
      "['lap', 'o']\n",
      "['drummer', 'y']\n",
      "['a', 'trembling']\n",
      "['drumm', 'y']\n",
      "['marrow', 'y']\n",
      "['betti', 'e']\n",
      "['mandolin', 'e']\n",
      "['a', 'pat']\n",
      "['a', 'hind']\n",
      "['mariner', 'e']\n",
      "['griff', 'e']\n",
      "['scars', 'e']\n",
      "['heel', 'e']\n",
      "['flew', 'e']\n",
      "['pray', 'e']\n",
      "['toil', 'e']\n",
      "['u', 'v']\n",
      "['flo', 'o']\n",
      "['a', 'next']\n",
      "['dan', 'y']\n",
      "['regal', 'e']\n",
      "['play', 'e']\n",
      "['away', 'e']\n",
      "['foam', 'e']\n",
      "['groan', 'e']\n",
      "['non', 'y']\n",
      "['ose', 'y']\n",
      "['spy', 'e']\n",
      "['moch', 'e']\n",
      "['regal', 'y']\n",
      "['vo', 'o']\n",
      "['e', 'claw']\n",
      "['peet', 'y']\n",
      "['prim', 'y']\n",
      "['stay', 'e']\n",
      "['jin', 'e']\n",
      "['marth', 'y']\n",
      "['koto', 'u']\n",
      "['marr', 'i']\n",
      "['e', 'paulette']\n",
      "['samp', 'o']\n",
      "['ex', 'e']\n",
      "['i', \"lion's\"]\n",
      "['hlad', 'e']\n",
      "['pop', 'o']\n",
      "['curtain', 'e']\n",
      "['skeel', 'y']\n",
      "['kaus', 'e']\n",
      "['mount', 'e']\n",
      "['sigh', 'e']\n",
      "['rio', 'u']\n",
      "['gossamer', 'y']\n",
      "['carlin', 'e']\n",
      "['sante', 'e']\n",
      "['tricks', 'y']\n",
      "['hong', 'e']\n",
      "['renown', 'e']\n",
      "['wathen', 'a']\n",
      "['winfred', 'a']\n",
      "['u', 'z']\n",
      "['y', 'allow']\n",
      "['lather', 'y']\n",
      "['plague', 'y']\n",
      "['a', 'merce']\n",
      "['a', 'wishing']\n",
      "['a', 'dust']\n",
      "['cush', 'a']\n",
      "['arrow', 'e']\n",
      "['shadow', 'e']\n",
      "['shepherd', 'e']\n",
      "['roof', 'e']\n",
      "['ebb', 'e']\n",
      "['spicer', 'y']\n",
      "['heather', 'y']\n",
      "['unread', 'y']\n",
      "['y', 'crested']\n",
      "['e', 'texts']\n",
      "['prosper', 'o']\n",
      "['mercer', 'y']\n",
      "['a', 'vouch']\n",
      "['nectar', 'y']\n",
      "['gomer', 'a']\n",
      "['spring', 'y']\n",
      "['wand', 'y']\n",
      "['florid', 'y']\n",
      "['prolong', 'e']\n",
      "['miss', 'e']\n",
      "['oar', 'y']\n",
      "['geffre', 'y']\n",
      "['fairness', 'e']\n",
      "['u', 'rania']\n",
      "['citi', 'e']\n",
      "['pipp', 'a']\n",
      "['y', 'alden']\n",
      "['shaff', 'y']\n",
      "['herb', 'e']\n",
      "['a', 'moretti']\n",
      "['halpin', 'e']\n",
      "['bead', 'e']\n",
      "['girl', 'e']\n",
      "['add', 'e']\n",
      "['buss', 'y']\n",
      "['winifred', 'a']\n",
      "['surgery', 'e']\n",
      "['malady', 'e']\n",
      "['month', 'e']\n",
      "['legend', 'e']\n",
      "['liberti', 'e']\n",
      "['wold', 'a']\n",
      "['sider', 'a']\n",
      "['deaf', 'e']\n",
      "['e', 'p']\n",
      "['gaub', 'e']\n",
      "['garland', 'e']\n",
      "['autumn', 'e']\n",
      "['reap', 'e']\n",
      "['y', 'old']\n",
      "['indeed', 'e']\n",
      "['dress', 'e']\n",
      "['undress', 'e']\n",
      "['access', 'e']\n",
      "['willingness', 'e']\n",
      "['first', 'e']\n",
      "['lack', 'y']\n",
      "['strength', 'y']\n",
      "['song', 'o']\n",
      "['cel', 'i']\n",
      "['vapor', 'y']\n",
      "['maj', 'a']\n",
      "['diner', 'o']\n",
      "['cur', 'a']\n",
      "['bled', 'a']\n",
      "['garo', 'u']\n",
      "['e', 'wa']\n",
      "['menn', 'i']\n",
      "['golding', 'e']\n",
      "['e', 'spousal']\n",
      "['walt', 'y']\n",
      "['a', 'freet']\n",
      "['siller', 'y']\n",
      "['other', 'e']\n",
      "['e', 'glamour']\n",
      "['monn', 'a']\n",
      "['veil', 'e']\n",
      "['ram', 'e']\n",
      "['brion', 'y']\n",
      "['a', 'tri']\n",
      "['ala', 'u']\n",
      "['plaudit', 'e']\n",
      "['pepin', 'o']\n",
      "['les', 'e']\n",
      "['rudolph', 'e']\n",
      "['chant', 'e']\n",
      "['altier', 'e']\n",
      "['e', 'tait']\n",
      "['robust', 'e']\n",
      "['just', 'e']\n",
      "['badour', 'a']\n",
      "['pens', 'o']\n",
      "['darr', 'o']\n",
      "['allah', 'u']\n",
      "['pilat', 'e']\n",
      "['e', 'querry']\n",
      "['clar', 'i']\n",
      "['und', 'e']\n",
      "['a', 'bunde']\n",
      "['hall', 'o']\n",
      "['porta', 'e']\n",
      "['port', 'u']\n",
      "['tut', 'o']\n",
      "['affect', 'o']\n",
      "['christ', 'e']\n",
      "['hill', 'o']\n",
      "['artist', 'a']\n",
      "['fond', 'i']\n",
      "['bastian', 'o']\n",
      "['i', 'tri']\n",
      "['marches', 'a']\n",
      "['philipp', 'o']\n",
      "['bacci', 'o']\n",
      "['valor', 'i']\n",
      "['rover', 'e']\n",
      "['luigi', 'a']\n",
      "['sebastian', 'o']\n",
      "['farnes', 'e']\n",
      "['monsignor', 'e']\n",
      "['bind', 'o']\n",
      "['gorin', 'i']\n",
      "['leones', 'e']\n",
      "['brag', 'e']\n",
      "['aud', 'e']\n",
      "['a', 'fries']\n",
      "['o', 'axes']\n",
      "['gossamer', 'e']\n",
      "['a', 'nodding']\n",
      "['whisper', 'y']\n",
      "['tows', 'e']\n",
      "['all', 'o']\n",
      "['homer', 'o']\n",
      "['latin', 'e']\n",
      "['eder', 'e']\n",
      "['e', 'o']\n",
      "['nesci', 'o']\n",
      "['academic', 'a']\n",
      "['diplomat', 'a']\n",
      "['a', \"waitin'\"]\n",
      "['wint', 'a']\n",
      "['e', 'our']\n",
      "['body', 'e']\n",
      "['e', 'leaven']\n",
      "['earthly', 'e']\n",
      "['i', 'hon']\n",
      "['blast', 'e']\n",
      "['kind', 'o']\n",
      "['sars', 'e']\n",
      "['e', 'end']\n",
      "['ane', 'y']\n",
      "['a', \"goin'\"]\n",
      "['wit', 'e']\n",
      "['digit', 'o']\n",
      "['a', 'page']\n",
      "['are', 'e']\n",
      "['er', 'y']\n",
      "['curios', 'e']\n",
      "['affectation', 'e']\n",
      "['verb', 'i']\n",
      "['ali', 'o']\n",
      "['attic', 'e']\n",
      "['a', \"lookin'\"]\n",
      "['a', 'ridge']\n",
      "['cellar', 'a']\n",
      "['liquor', 'e']\n",
      "['horrid', 'a']\n",
      "['coleman', 'e']\n",
      "['tyler', 'e']\n",
      "['oblivion', 'e']\n",
      "['cord', 'i']\n",
      "['imm', 'o']\n",
      "['e', 'jus']\n",
      "['pocket', 'a']\n",
      "['rapt', 'a']\n",
      "['us', 'u']\n",
      "['inept', 'i']\n",
      "['non', 'o']\n",
      "['licker', 'e']\n",
      "['ali', 'i']\n",
      "['braz', 'e']\n",
      "['a', 'dance']\n",
      "['vo', 'y']\n",
      "['hob', 'y']\n",
      "['mehrer', 'e']\n",
      "['kurz', 'e']\n",
      "['leicht', 'e']\n",
      "['reim', 'e']\n",
      "['pseudonym', 'e']\n",
      "['flit', 'o']\n",
      "['o', 'di']\n",
      "['dr', 'i']\n",
      "['med', 'u']\n",
      "['foresta', 'y']\n",
      "['fidel', 'e']\n",
      "['macneil', 'e']\n",
      "['mackay', 'e']\n",
      "['haws', 'e']\n",
      "['a', 'quiver']\n",
      "['ferm', 'e']\n",
      "['bello', 'y']\n",
      "['darin', 'o']\n",
      "['e', 'qualities']\n",
      "['spirit', 'a']\n",
      "['flour', 'y']\n",
      "['sonn', 'o']\n",
      "['piet', 'a']\n",
      "['que', 'i']\n",
      "['covert', 'a']\n",
      "['lung', 'o']\n",
      "['pare', 'a']\n",
      "['stil', 'o']\n",
      "['pols', 'i']\n",
      "['spirit', 'i']\n",
      "['second', 'a']\n",
      "['grid', 'a']\n",
      "['o', 'mo']\n",
      "['intend', 'i']\n",
      "['guard', 'i']\n",
      "['ard', 'i']\n",
      "['rachel', 'e']\n",
      "['person', 'e']\n",
      "['dann', 'o']\n",
      "['cota', 'i']\n",
      "['qual', 'i']\n",
      "['gel', 'o']\n",
      "['vo', 'i']\n",
      "['volt', 'o']\n",
      "['mis', 'e']\n",
      "['alt', 'i']\n",
      "['sanz', 'a']\n",
      "['accent', 'i']\n",
      "['tumult', 'o']\n",
      "['tint', 'a']\n",
      "['guard', 'a']\n",
      "['pass', 'a']\n",
      "['viv', 'i']\n",
      "['i', 'vi']\n",
      "['got', 'e']\n",
      "['livid', 'a']\n",
      "['ave', 'a']\n",
      "['color', 'e']\n",
      "['dent', 'i']\n",
      "['tutt', 'e']\n",
      "['quant', 'e']\n",
      "['spogli', 'e']\n",
      "['lit', 'o']\n",
      "['moss', 'i']\n",
      "['fis', 'o']\n",
      "['foss', 'i']\n",
      "['fond', 'o']\n",
      "['second', 'o']\n",
      "['molt', 'i']\n",
      "['beat', 'i']\n",
      "['sapp', 'i']\n",
      "['ess', 'i']\n",
      "['dic', 'o']\n",
      "['lung', 'a']\n",
      "['vinci', 'a']\n",
      "['intra', 'i']\n",
      "['tutt', 'a']\n",
      "['confess', 'a']\n",
      "['rest', 'a']\n",
      "['molest', 'a']\n",
      "['u', 'dire']\n",
      "['tac', 'e']\n",
      "['pres', 'e']\n",
      "['cain', 'a']\n",
      "['spens', 'e']\n",
      "['quest', 'e']\n",
      "['torment', 'i']\n",
      "['grandin', 'e']\n",
      "['gross', 'a']\n",
      "['rivers', 'a']\n",
      "['can', 'i']\n",
      "['lat', 'i']\n",
      "['fann', 'o']\n",
      "['a', 'perse']\n",
      "['ferm', 'o']\n",
      "['fier', 'a']\n",
      "['ah', 'i']\n",
      "['tant', 'e']\n",
      "['nov', 'e']\n",
      "['pen', 'e']\n",
      "['pap', 'i']\n",
      "['co', 'i']\n",
      "['post', 'i']\n",
      "['van', 'i']\n",
      "['senn', 'i']\n",
      "['pett', 'o']\n",
      "['bran', 'o']\n",
      "['senn', 'o']\n",
      "['mort', 'a']\n",
      "['pass', 'i']\n",
      "['for', 'i']\n",
      "['superb', 'o']\n",
      "['pastor', 'i']\n",
      "['ran', 'e']\n",
      "['aer', 'e']\n",
      "['torment', 'o']\n",
      "['hann', 'o']\n",
      "['ell', 'i']\n",
      "['o', 'do']\n",
      "['tenet', 'e']\n",
      "['pied', 'e']\n",
      "['aver', 'e']\n",
      "['ruin', 'e']\n",
      "['tollett', 'e']\n",
      "['schier', 'e']\n",
      "['quell', 'e']\n",
      "['offend', 'e']\n",
      "['molt', 'e']\n",
      "['foss', 'a']\n",
      "['tort', 'a']\n",
      "['schier', 'a']\n",
      "['prod', 'a']\n",
      "['pung', 'e']\n",
      "['cornet', 'o']\n",
      "['cap', 'i']\n",
      "['gem', 'e']\n",
      "['putt', 'i']\n",
      "['corp', 'i']\n",
      "['post', 'a']\n",
      "['frasch', 'e']\n",
      "['lett', 'o']\n",
      "['gregg', 'e']\n",
      "['divers', 'a']\n",
      "['stanch', 'i']\n",
      "['vulcan', 'o']\n",
      "['diss', 'i']\n",
      "['debit', 'i']\n",
      "['margin', 'i']\n",
      "['pass', 'o']\n",
      "['sent', 'a']\n",
      "['reverent', 'e']\n",
      "['malign', 'o']\n",
      "['tien', 'e']\n",
      "['lung', 'i']\n",
      "['stram', 'e']\n",
      "['bram', 'a']\n",
      "['color', 'o']\n",
      "['corron', 'o']\n",
      "['frett', 'a']\n",
      "['pres', 'a']\n",
      "['stat', 'o']\n",
      "['sare', 'i']\n",
      "['face', 'a']\n",
      "['pom', 'i']\n",
      "['pri', 'a']\n",
      "['tom', 'i']\n",
      "['levant', 'e']\n",
      "['press', 'o']\n",
      "['sus', 'o']\n",
      "['serpent', 'e']\n",
      "['fust', 'o']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doss', 'o']\n",
      "['nod', 'i']\n",
      "['coll', 'o']\n",
      "['bors', 'a']\n",
      "['spent', 'a']\n",
      "['lent', 'a']\n",
      "['vent', 'a']\n",
      "['cent', 'o']\n",
      "['pon', 'e']\n",
      "['fell', 'o']\n",
      "['quell', 'i']\n",
      "['colt', 'o']\n",
      "['vann', 'o']\n",
      "['suon', 'i']\n",
      "['mag', 'o']\n",
      "['esser', 'e']\n",
      "['piatt', 'i']\n",
      "['viet', 'a']\n",
      "['cole', 'i']\n",
      "['reg', 'i']\n",
      "['test', 'e']\n",
      "['pote', 'a']\n",
      "['ru', 'i']\n",
      "['distend', 'e']\n",
      "['crud', 'a']\n",
      "['pent', 'e']\n",
      "['mali', 'e']\n",
      "['avant', 'e']\n",
      "['inter', 'o']\n",
      "['castell', 'a']\n",
      "['demon', 'i']\n",
      "['mis', 'i']\n",
      "['donn', 'o']\n",
      "['stanch', 'e']\n",
      "['voland', 'o']\n",
      "['tacit', 'i']\n",
      "['intent', 'i']\n",
      "['pres', 'i']\n",
      "['stent', 'a']\n",
      "['brin', 'a']\n",
      "['rob', 'a']\n",
      "[\"d'or\", 'a']\n",
      "['ruin', 'a']\n",
      "['manifest', 'a']\n",
      "['a', 'pri']\n",
      "['gropp', 'a']\n",
      "['frate', 'i']\n",
      "['arment', 'o']\n",
      "['vist', 'e']\n",
      "['norm', 'e']\n",
      "['fess', 'e']\n",
      "['sal', 'i']\n",
      "['poggi', 'o']\n",
      "['dic', 'a']\n",
      "['ebb', 'i']\n",
      "['valor', 'e']\n",
      "['brut', 'i']\n",
      "['ard', 'o']\n",
      "['confess', 'o']\n",
      "['rende', 'i']\n",
      "['foss', 'o']\n",
      "['verri', 'a']\n",
      "['sermon', 'e']\n",
      "['o', 'recchia']\n",
      "['sommers', 'e']\n",
      "['animal', 'i']\n",
      "['poet', 'i']\n",
      "['e', 'cuba']\n",
      "['miser', 'a']\n",
      "['passat', 'i']\n",
      "['nat', 'i']\n",
      "['voll', 'i']\n",
      "['bram', 'o']\n",
      "['canal', 'i']\n",
      "['moll', 'i']\n",
      "['dare', 'i']\n",
      "['miser', 'o']\n",
      "['torr', 'i']\n",
      "['rott', 'o']\n",
      "['raun', 'a']\n",
      "['rod', 'o']\n",
      "['mud', 'a']\n",
      "['scan', 'e']\n",
      "['dest', 'o']\n",
      "['mess', 'o']\n",
      "['vis', 'i']\n",
      "['mang', 'i']\n",
      "['miser', 'e']\n",
      "['spogli', 'a']\n",
      "['dove', 'i']\n",
      "['porr', 'e']\n",
      "['distend', 'i']\n",
      "['cortesi', 'a']\n",
      "['buc', 'a']\n",
      "['ros', 'o']\n",
      "['list', 'a']\n",
      "['torn', 'a']\n",
      "['prim', 'i']\n",
      "['penn', 'e']\n",
      "['capricorn', 'o']\n",
      "['novell', 'e']\n",
      "['oh', 'i']\n",
      "['pins', 'i']\n",
      "['colomb', 'i']\n",
      "['stann', 'o']\n",
      "['rott', 'a']\n",
      "['mortal', 'i']\n",
      "['allor', 'a']\n",
      "['lett', 'a']\n",
      "['snell', 'e']\n",
      "['lit', 'i']\n",
      "['alza', 'i']\n",
      "['equator', 'e']\n",
      "['attend', 'i']\n",
      "['rec', 'a']\n",
      "['divis', 'a']\n",
      "['dice', 'a']\n",
      "['rider', 'e']\n",
      "['lombard', 'a']\n",
      "['serv', 'a']\n",
      "['prod', 'e']\n",
      "['giudici', 'o']\n",
      "['fil', 'i']\n",
      "['monet', 'a']\n",
      "['merit', 'o']\n",
      "['st', 'o']\n",
      "['ress', 'e']\n",
      "['molt', 'a']\n",
      "['ream', 'i']\n",
      "['pai', 'a']\n",
      "['vest', 'e']\n",
      "['fe', 'i']\n",
      "['valle', 'a']\n",
      "['porta', 'i']\n",
      "['animal', 'e']\n",
      "[\"d'or\", 'o']\n",
      "['balz', 'o']\n",
      "['grad', 'i']\n",
      "['divers', 'i']\n",
      "['died', 'i']\n",
      "['err', 'i']\n",
      "['cardin', 'i']\n",
      "['distort', 'i']\n",
      "['scars', 'i']\n",
      "['ann', 'i']\n",
      "['move', 'a']\n",
      "['content', 'i']\n",
      "['lent', 'i']\n",
      "['superb', 'i']\n",
      "['lass', 'i']\n",
      "['render', 'o']\n",
      "['manifest', 'e']\n",
      "['superb', 'a']\n",
      "['dom', 'a']\n",
      "['nom', 'a']\n",
      "['sall', 'o']\n",
      "['fant', 'e']\n",
      "['port', 'i']\n",
      "['mort', 'i']\n",
      "['fis', 'i']\n",
      "['par', 'i']\n",
      "['timbre', 'o']\n",
      "['romp', 'e']\n",
      "['fog', 'a']\n",
      "['scale', 'e']\n",
      "['dog', 'a']\n",
      "['letter', 'e']\n",
      "['dismal', 'a']\n",
      "['scald', 'i']\n",
      "['pont', 'a']\n",
      "['duc', 'i']\n",
      "['dann', 'i']\n",
      "['leva', 'i']\n",
      "['salt', 'a']\n",
      "['vesper', 'o']\n",
      "['lucent', 'i']\n",
      "['modern', 'o']\n",
      "['us', 'o']\n",
      "['corr', 'e']\n",
      "['reg', 'e']\n",
      "['sole', 'a']\n",
      "['brig', 'a']\n",
      "['vapor', 'i']\n",
      "['u', 'midi']\n",
      "['butt', 'o']\n",
      "['giron', 'e']\n",
      "['contend', 'e']\n",
      "['timid', 'o']\n",
      "['batt', 'i']\n",
      "['pote', 'i']\n",
      "['punit', 'a']\n",
      "['merl', 'i']\n",
      "['sari', 'a']\n",
      "['big', 'i']\n",
      "['dot', 'a']\n",
      "['fiel', 'e']\n",
      "['i', 'r']\n",
      "['convent', 'o']\n",
      "['talent', 'o']\n",
      "['pres', 'o']\n",
      "['lunar', 'i']\n",
      "['sermon', 'i']\n",
      "['cade', 'a']\n",
      "['pom', 'o']\n",
      "['error', 'e']\n",
      "['moves', 'i']\n",
      "['vass', 'i']\n",
      "['bev', 'e']\n",
      "['mens', 'a']\n",
      "['prem', 'e']\n",
      "['cast', 'i']\n",
      "['bast', 'i']\n",
      "['past', 'i']\n",
      "['stupid', 'o']\n",
      "['stupor', 'e']\n",
      "['ve', 'i']\n",
      "['jo', 'i']\n",
      "['divis', 'e']\n",
      "['poch', 'i']\n",
      "['oma', 'i']\n",
      "['salm', 'o']\n",
      "['convert', 'a']\n",
      "['distant', 'e']\n",
      "['mist', 'e']\n",
      "['nod', 'o']\n",
      "['sod', 'o']\n",
      "['band', 'o']\n",
      "['cavern', 'a']\n",
      "['rendon', 'o']\n",
      "['inter', 'a']\n",
      "['tropp', 'a']\n",
      "['ret', 'e']\n",
      "['robust', 'o']\n",
      "['i', 'arba']\n",
      "['vint', 'o']\n",
      "['fiss', 'i']\n",
      "['sens', 'i']\n",
      "['spent', 'i']\n",
      "['turgid', 'e']\n",
      "['fans', 'i']\n",
      "['ping', 'a']\n",
      "['vint', 'i']\n",
      "['sonn', 'i']\n",
      "['rott', 'i']\n",
      "['stars', 'i']\n",
      "['laid', 'e']\n",
      "['corn', 'o']\n",
      "['crud', 'o']\n",
      "['scud', 'o']\n",
      "['reverent', 'i']\n",
      "['bui', 'a']\n",
      "['stat', 'i']\n",
      "['gels', 'a']\n",
      "['croc', 'i']\n",
      "['govern', 'i']\n",
      "['vives', 'i']\n",
      "['equal', 'e']\n",
      "['solid', 'a']\n",
      "['adamant', 'e']\n",
      "['dimension', 'e']\n",
      "['rep', 'e']\n",
      "['lum', 'i']\n",
      "['volt', 'i']\n",
      "['ingest', 'o']\n",
      "['torn', 'i']\n",
      "['prima', 'i']\n",
      "['different', 'i']\n",
      "['format', 'i']\n",
      "['distant', 'i']\n",
      "['condescend', 'e']\n",
      "['intend', 'e']\n",
      "['grad', 'a']\n",
      "['sever', 'o']\n",
      "['testament', 'o']\n",
      "['string', 'e']\n",
      "['dec', 'i']\n",
      "['fab', 'i']\n",
      "['occident', 'e']\n",
      "['gian', 'o']\n",
      "['fall', 'i']\n",
      "['fiss', 'o']\n",
      "['dic', 'i']\n",
      "['fess', 'i']\n",
      "['grid', 'o']\n",
      "['coste', 'i']\n",
      "['set', 'a']\n",
      "['caton', 'a']\n",
      "['nascent', 'e']\n",
      "['accor', 'a']\n",
      "['parc', 'a']\n",
      "['metter', 'e']\n",
      "['arc', 'a']\n",
      "['content', 'a']\n",
      "['splendor', 'i']\n",
      "['pi', 'i']\n",
      "['cort', 'o']\n",
      "['vatican', 'o']\n",
      "['quart', 'a']\n",
      "['tacit', 'e']\n",
      "['cor', 'o']\n",
      "['offers', 'e']\n",
      "['temp', 'i']\n",
      "['u', 'baldo']\n",
      "['conversion', 'e']\n",
      "['ancell', 'a']\n",
      "['dove', 'a']\n",
      "['vacant', 'e']\n",
      "['torrent', 'e']\n",
      "['big', 'a']\n",
      "['cup', 'e']\n",
      "['ferm', 'a']\n",
      "['manier', 'a']\n",
      "['concord', 'i']\n",
      "['intend', 'o']\n",
      "['rigid', 'o']\n",
      "['offerer', 'e']\n",
      "['mun', 'o']\n",
      "['car', 'i']\n",
      "['movers', 'i']\n",
      "['os', 'a']\n",
      "['nerl', 'i']\n",
      "['elise', 'o']\n",
      "['pad', 'o']\n",
      "['usurp', 'a']\n",
      "['fall', 'o']\n",
      "['fiat', 'e']\n",
      "['lun', 'i']\n",
      "['it', 'e']\n",
      "['pall', 'e']\n",
      "['raun', 'i']\n",
      "['ogg', 'i']\n",
      "['lamp', 'a']\n",
      "['precis', 'o']\n",
      "['parti', 'o']\n",
      "['i', 'polito']\n",
      "['am', 'a']\n",
      "['dover', 'e']\n",
      "['soler', 'e']\n",
      "['stolt', 'i']\n",
      "['ream', 'e']\n",
      "['seder', 'e']\n",
      "['spann', 'a']\n",
      "['ardent', 'e']\n",
      "['lucid', 'i']\n",
      "['angelic', 'i']\n",
      "['feces', 'i']\n",
      "['frond', 'a']\n",
      "['tron', 'o']\n",
      "['scale', 'o']\n",
      "['gel', 'i']\n",
      "['modern', 'i']\n",
      "['men', 'i']\n",
      "['prob', 'o']\n",
      "['format', 'a']\n",
      "['tend', 'e']\n",
      "['tent', 'a']\n",
      "['latent', 'e']\n",
      "['profession', 'e']\n",
      "['guasch', 'i']\n",
      "['amor', 'i']\n",
      "['tron', 'i']\n",
      "['motor', 'i']\n",
      "['fur', 'o']\n",
      "['fass', 'i']\n",
      "['bali', 'a']\n",
      "['cant', 'i']\n",
      "['masch', 'i']\n",
      "['poss', 'a']\n",
      "['avers', 'i']\n",
      "['color', 'i']\n",
      "['jug', 'a']\n",
      "['castell', 'e']\n",
      "['bristow', 'e']\n",
      "['john', 'e']\n",
      "['account', 'e']\n",
      "['match', 'e']\n",
      "['posterity', 'e']\n",
      "['bott', 'e']\n",
      "['twain', 'e']\n",
      "['strait', 'e']\n",
      "['blood', 'e']\n",
      "['full', 'e']\n",
      "['stool', 'e']\n",
      "['my', 'e']\n",
      "['guard', 'e']\n",
      "['thy', 'e']\n",
      "['taught', 'e']\n",
      "['kenn', 'e']\n",
      "['who', 'e']\n",
      "['sai', 'e']\n",
      "['sledd', 'e']\n",
      "['drawn', 'e']\n",
      "['adversity', 'e']\n",
      "['rott', 'e']\n",
      "['gould', 'e']\n",
      "['should', 'e']\n",
      "['bled', 'e']\n",
      "['tempt', 'e']\n",
      "['last', 'e']\n",
      "['left', 'e']\n",
      "['wier', 'e']\n",
      "['birth', 'a']\n",
      "['edward', 'e']\n",
      "['their', 'e']\n",
      "['shall', 'e']\n",
      "['trump', 'e']\n",
      "['soll', 'e']\n",
      "['abound', 'e']\n",
      "['content', 'e']\n",
      "['aldin', 'e']\n",
      "['dern', 'e']\n",
      "['mok', 'y']\n",
      "['stai', 'e']\n",
      "['aborn', 'e']\n",
      "['opp', 'e']\n",
      "['strung', 'e']\n",
      "['onto', 'e']\n",
      "['lough', 'e']\n",
      "['alys', 'e']\n",
      "['would', 'e']\n",
      "['knopp', 'e']\n",
      "['hann', 'e']\n",
      "['exhibit', 'a']\n",
      "['author', 'i']\n",
      "['faller', 'e']\n",
      "['spic', 'a']\n",
      "['portion', 'e']\n",
      "['fe', 'u']\n",
      "['segment', 'o']\n",
      "['y', 'made']\n",
      "['a', 'throw']\n",
      "['fat', 'o']\n",
      "['a', 'come']\n",
      "['toller', 'e']\n",
      "['cherub', 'e']\n",
      "['gulf', 'e']\n",
      "['soil', 'e']\n",
      "['seis', 'e']\n",
      "['maw', 'e']\n",
      "['redeem', 'e']\n",
      "['outdo', 'o']\n",
      "['join', 'e']\n",
      "['malign', 'e']\n",
      "['orb', 'e']\n",
      "['a', 'egypt']\n",
      "['design', 'e']\n",
      "['e', 'even']\n",
      "['discern', 'e']\n",
      "['vari', 'e']\n",
      "['myrrh', 'e']\n",
      "['balm', 'e']\n",
      "['condemn', 'e']\n",
      "['dawn', 'e']\n",
      "['lax', 'e']\n",
      "['ordain', 'e']\n",
      "['mankind', 'e']\n",
      "['concern', 'e']\n",
      "['sunni', 'e']\n",
      "['sign', 'e']\n",
      "['forgo', 'e']\n",
      "['pri', 'e']\n",
      "['esteem', 'e']\n",
      "['avoid', 'e']\n",
      "['reli', 'e']\n",
      "['sail', 'e']\n",
      "['bann', 'e']\n",
      "['undergo', 'e']\n",
      "['quarre', 'y']\n",
      "['centaur', 'e']\n",
      "['steam', 'e']\n",
      "['wisdom', 'e']\n",
      "['impair', 'e']\n",
      "['den', 'e']\n",
      "['sign', 'y']\n",
      "['a', 'post']\n",
      "['goldwin', 'e']\n",
      "['espy', 'e']\n",
      "['e', 'al']\n",
      "['gum', 'a']\n",
      "['bent', 'y']\n",
      "['blanck', 'e']\n",
      "['step', 'e']\n",
      "['st', 'e']\n",
      "['e', 'alles']\n",
      "['heim', 'e']\n",
      "['sun', 'u']\n",
      "['u', 'ton']\n",
      "['nix', 'e']\n",
      "['heard', 'a']\n",
      "['u', 'ten']\n",
      "['sax', 'o']\n",
      "['a', 'tol']\n",
      "['word', 'a']\n",
      "['ban', 'a']\n",
      "['heals', 'e']\n",
      "['beg', 'a']\n",
      "['carer', 'e']\n",
      "['bead', 'u']\n",
      "['beal', 'u']\n",
      "['bear', 'u']\n",
      "['sel', 'e']\n",
      "['bog', 'a']\n",
      "['bracht', 'e']\n",
      "['e', 'ode']\n",
      "['cemp', 'a']\n",
      "['a', 'tolan']\n",
      "['rand', 'e']\n",
      "['camp', 'e']\n",
      "['gear', 'e']\n",
      "['lag', 'u']\n",
      "['e', 'orla']\n",
      "['dorst', 'e']\n",
      "['ent', 'a']\n",
      "['e', 'all']\n",
      "['e', 'allum']\n",
      "['e', 'alu']\n",
      "['e', 'axle']\n",
      "['e', 'ofer']\n",
      "['fat', 'u']\n",
      "['facer', 'e']\n",
      "['fund', 'e']\n",
      "['fir', 'a']\n",
      "['sig', 'e']\n",
      "['bead', 'o']\n",
      "['gear', 'o']\n",
      "['fremd', 'e']\n",
      "['frum', 'a']\n",
      "['wit', 'a']\n",
      "['o', 'lim']\n",
      "['sear', 'o']\n",
      "['grund', 'e']\n",
      "['si', 'o']\n",
      "['heard', 'e']\n",
      "['hird', 'e']\n",
      "['leng', 'e']\n",
      "['bequeath', 'e']\n",
      "['mag', 'a']\n",
      "['wiss', 'e']\n",
      "['nam', 'a']\n",
      "['sorg', 'a']\n",
      "['nicer', 'a']\n",
      "['spell', 'a']\n",
      "['ord', 'e']\n",
      "['mechanic', 'a']\n",
      "['roder', 'a']\n",
      "['sac', 'u']\n",
      "['scad', 'u']\n",
      "['sigl', 'u']\n",
      "['streng', 'o']\n",
      "['streng', 'e']\n",
      "['sigl', 'a']\n",
      "['beal', 'o']\n",
      "['wang', 'e']\n",
      "['wong', 'e']\n",
      "['wal', 'u']\n",
      "['cum', 'a']\n",
      "['worn', 'a']\n",
      "['gest', 'a']\n",
      "['fund', 'i']\n",
      "['barr', 'i']\n",
      "['goat', 'y']\n",
      "['vari', 'a']\n",
      "['in', 'o']\n",
      "['colebrook', 'e']\n",
      "['hers', 'e']\n",
      "['turf', 'y']\n",
      "['curd', 'y']\n",
      "['paradox', 'e']\n",
      "['text', 'e']\n",
      "['fait', 'e']\n",
      "['quo', 'i']\n",
      "['fill', 'e']\n",
      "['fass', 'e']\n",
      "['prenn', 'e']\n",
      "['superb', 'e']\n",
      "['tig', 'e']\n",
      "['a', 'fin']\n",
      "['plein', 'e']\n",
      "['vast', 'e']\n",
      "['sort', 'i']\n",
      "['sera', 'i']\n",
      "['origin', 'e']\n",
      "['tartar', 'e']\n",
      "['odorant', 'e']\n",
      "['plair', 'e']\n",
      "['absurd', 'e']\n",
      "['o', 'vide']\n",
      "['employ', 'a']\n",
      "['par', 'u']\n",
      "['serv', 'i']\n",
      "['arm', 'a']\n",
      "['artist', 'e']\n",
      "['hard', 'i']\n",
      "['transport', 'e']\n",
      "['blossom', 'y']\n",
      "['camm', 'a']\n",
      "['o', 'elsner']\n",
      "['jock', 'o']\n",
      "['beel', 'a']\n",
      "['hippert', 'y']\n",
      "['y', 'esler']\n",
      "['hink', 'y']\n",
      "['mull', 'y']\n",
      "['tagg', 'y']\n",
      "['nice', 'y']\n",
      "['stare', 'y']\n",
      "['chapp', 'y']\n",
      "['coat', 'i']\n",
      "['raab', 'a']\n",
      "['streak', 'y']\n",
      "['menn', 'y']\n",
      "['vous', 'e']\n",
      "['gunn', 'e']\n",
      "['jock', 'e']\n",
      "['picard', 'y']\n",
      "['bin', 'e']\n",
      "['fitz', 'y']\n",
      "['formula', 'e']\n",
      "['piece', 'e']\n",
      "['snarl', 'y']\n",
      "['bank', 'y']\n",
      "['prom', 'e']\n",
      "['tidd', 'y']\n",
      "['bole', 'e']\n",
      "['sil', 'e']\n",
      "['georgi', 'i']\n",
      "['quint', 'i']\n",
      "['than', 'a']\n",
      "['gild', 'e']\n",
      "['relig', 'i']\n",
      "['u', 'shant']\n",
      "['waga', 'i']\n",
      "['ikon', 'a']\n",
      "['jo', 'u']\n",
      "['aha', 'e']\n",
      "['karel', 'a']\n",
      "['enjoy', 'a']\n",
      "['tar', 'i']\n",
      "['drives', 'e']\n",
      "['spath', 'e']\n",
      "['hindustan', 'i']\n",
      "['hirt', 'e']\n",
      "['average', 'e']\n",
      "['mh', 'e']\n",
      "['a', 'basements']\n",
      "['ripple', 'e']\n",
      "['a', 'squeal']\n",
      "['plunk', 'a']\n",
      "['link', 'a']\n",
      "['tb', 'e']\n",
      "['cambra', 'i']\n",
      "['blay', 'e']\n",
      "['kitchen', 'y']\n",
      "['scherz', 'o']\n",
      "['saad', 'i']\n",
      "['skill', 'y']\n",
      "['mun', 'e']\n",
      "['dent', 'y']\n",
      "['shun', 'e']\n",
      "['lot', 'u']\n",
      "['aix', 'e']\n",
      "['a', 'fit']\n",
      "['y', 'irks']\n",
      "['tent', 'y']\n",
      "['surf', 'y']\n",
      "['omar', 'e']\n",
      "['ur', 'a']\n",
      "['pae', 'a']\n",
      "['mara', 'e']\n",
      "['tap', 'u']\n",
      "['copartner', 'y']\n",
      "['aston', 'y']\n",
      "['schul', 'e']\n",
      "['rabbi', 'e']\n",
      "['tick', 'y']\n",
      "['lxi', 'i']\n",
      "['las', 'y']\n",
      "['colling', 'a']\n",
      "['sar', 'o']\n",
      "['y', 'atha']\n",
      "['gulp', 'y']\n",
      "['trout', 'y']\n",
      "['jopp', 'a']\n",
      "['outcast', 'e']\n",
      "['loo', 'e']\n",
      "['calla', 'o']\n",
      "['au', 'a']\n",
      "['oh', 'a']\n",
      "['hoot', 'y']\n",
      "['pops', 'y']\n",
      "['dumb', 'e']\n",
      "['maine', 'e']\n",
      "['o', 'presses']\n",
      "['legg', 'o']\n",
      "['litt', 'e']\n",
      "['fix', 'e']\n",
      "['o', 'esophagus']\n",
      "['verrier', 'e']\n",
      "['nook', 'y']\n",
      "['a', 'flaming']\n",
      "['o', 'bove']\n",
      "['gumbi', 'e']\n",
      "['wat', 'e']\n",
      "['a', 'drizzle']\n",
      "['medusa', 'e']\n",
      "['saba', 'a']\n",
      "['pell', 'y']\n",
      "['tank', 'a']\n",
      "['wo', 'u']\n",
      "['e', 'vanishing']\n",
      "['remis', 'e']\n",
      "['fenn', 'y']\n",
      "['corvin', 'e']\n",
      "['tromp', 'e']\n",
      "['shimmer', 'y']\n",
      "['a', 'lurk']\n",
      "['mouse', 'y']\n",
      "['e', 'edge']\n",
      "['herb', 'a']\n",
      "['mais', 'y']\n",
      "['clapped', 'e']\n",
      "['milliner', 'y']\n",
      "['a', 'past']\n",
      "['proud', 'y']\n",
      "['mystere', 'y']\n",
      "['mash', 'y']\n",
      "['boil', 'y']\n",
      "['a', 'flying']\n",
      "['mountain', 'y']\n",
      "['a', 'shimmer']\n",
      "['tutt', 'y']\n",
      "['a', 'vore']\n",
      "['hurt', 'y']\n",
      "['cordero', 'y']\n",
      "['collier', 'y']\n",
      "['woll', 'y']\n",
      "['shinn', 'y']\n",
      "['i', 'ga']\n",
      "['o', 'l']\n",
      "[\"didn't\", 'u']\n"
     ]
    }
   ],
   "source": [
    "splitwords = findwordsplits(unknown_words)\n",
    "# inspect what words have single letters as vowels to determine appropriate sounds\n",
    "vowels = ['a', 'e', 'i','o', 'u', 'y']\n",
    "for i in splitwords:\n",
    "    if (len(splitwords[i][0]) ==1 or len(splitwords[i][1]) ==1):\n",
    "        if(splitwords[i][0] in vowels or splitwords[i][1] in vowels):\n",
    "            print(splitwords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle words with single letters separately\n",
    "# Note: this may not always be accruacte but should approximate well enough for the intended goal\n",
    "def convertsplitwords(splitwords):\n",
    "    worddict = copy.deepcopy(splitwords)\n",
    "    special_cases = { \n",
    "        'a':'AH0',\n",
    "        'i': 'IY0',\n",
    "        'o': 'OW0',\n",
    "        'u' : 'UW0',\n",
    "        'y':'IY0',\n",
    "        's': 'Z'\n",
    "    }\n",
    "    \n",
    "    for i in worddict:\n",
    "        \n",
    "        firstword = worddict[i][0]\n",
    "        secondword = worddict[i][1]\n",
    "\n",
    "        # If the first word is a single letter then make it all caps\n",
    "        if (len(firstword)==1):\n",
    "            firstword = firstword.upper()\n",
    "            \n",
    "        # Otherwise use CMUDICT to convert\n",
    "        else:\n",
    "            firstword = getphoneme(firstword)\n",
    "\n",
    "        # If the second word is a single letter then apply the special cases where applicable\n",
    "        if (len(secondword)==1):\n",
    "            \n",
    "            # If letter is a special case replace it\n",
    "            if secondword in special_cases.keys():\n",
    "                secondword = pd.Series(worddict[i]).replace(special_cases)[1]\n",
    "                \n",
    "            # Otherwise make it all caps\n",
    "            else:\n",
    "                secondword = secondword.upper()\n",
    "                \n",
    "       # Otherwise use CMUDICT to convert\n",
    "        else:\n",
    "            secondword = getphoneme(secondword)\n",
    "            \n",
    "        # added nested list to match format of CMUDICT\n",
    "        worddict[i] = [[firstword,secondword]]   \n",
    "        \n",
    "    return worddict\n",
    "\n",
    "# takes in haikus and separates haikus that have words not in the CMUDICT\n",
    "def splithaikus(haikus):\n",
    "    \n",
    "    bad_haikus = []\n",
    "    good_haikus = []\n",
    "    \n",
    "    for haiku in haikus:\n",
    "        words = getwords(haiku)\n",
    "        if all(word in cmudict.keys() for word in words):\n",
    "            good_haikus.append(haiku)\n",
    "        else:\n",
    "            bad_haikus.append(haiku)\n",
    "                        \n",
    "    return bad_haikus, good_haikus\n",
    "\n",
    "# Takes in a haiku and transforms it into the equivalent phoneme version\n",
    "def haikutransform(haiku):\n",
    "    words = getwords(haiku)\n",
    "    try:\n",
    "        phonemes = [* map(getphoneme,words)]\n",
    "        phonemes = ' '.join(phonemes)\n",
    "    except:\n",
    "        raise ValueError('A word in the Haiku was not in the CMUDICT.' \\\n",
    "        ' Make sure only valid haikus are used for this function.')\n",
    "        return\n",
    "        \n",
    "    return phonemes\n",
    "\n",
    "# CMUDICT uses numbers (0-2) to denote stress of the syllable. Although this is something that could be explored\n",
    "# later, it is adding unnecessary complexity and should be changed to a consistent format. \n",
    "def convertsyllables(haiku):\n",
    "    haiku = re.sub('(1|2)','0',haiku)\n",
    "    return haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython/7.5.0/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# add new word phoenetics to cmudict\n",
    "cmudict = {**cmudict ,**convertsplitwords(splitwords)}\n",
    "# split haikus into a usuable set and a set that can be further inspected for transformations\n",
    "bad_haikus, valid_haikus = splithaikus(haikus)\n",
    "# transform the good haikus into phonemes\n",
    "haikus_transformed = pd.Series(map(haikutransform,valid_haikus))\n",
    "# Convert all syllables to 0\n",
    "haikus_transformed = pd.Series(map(convertsyllables,haikus_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if we didnt lose any haikus\n",
    "len(bad_haikus) + len(haikus_transformed) == len(haikus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Function to Convert Back to Regular English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the cmudict so that we can transform from a phoneme to english\n",
    "def invertdictionary(cmudict):\n",
    "    \n",
    "    idict = {}\n",
    "    \n",
    "    for word in cmudict:\n",
    "        # Create a list to hold all of the possible words associated with a phoneme\n",
    "        p_list = []\n",
    "        \n",
    "        phoneme = ''.join(cmudict[word][0]) # always use first phoneme for a word\n",
    "        phoneme = convertsyllables(phoneme)\n",
    "        \n",
    "        # if the phoneme already exists add it to that list\n",
    "        if phoneme in idict.keys():\n",
    "            p_list = idict[phoneme]\n",
    "            p_list.append(word)\n",
    "            \n",
    "        # Otherwise create a new list\n",
    "        else:\n",
    "            p_list.append(word)\n",
    "           \n",
    "        idict[phoneme] = p_list\n",
    "    return idict\n",
    "\n",
    "#def getenglish(phoneme):\n",
    " #   words = idict[phoneme][0] # always use first pronuciations at index 0\n",
    "                                         # This will be explored in the future to optimize word choice\n",
    "  #  return words\n",
    "\n",
    "def transformback(haiku):\n",
    "    # use getwords to get phonemes\n",
    "    phonemes = getwords(haiku)\n",
    "    words = [* map(getenglish,phonemes)]\n",
    "    words = ' '.join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Inverted Phoneme Dictionary\n",
    "idict = invertdictionary(copy.deepcopy(cmudict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes in haikus and predicts what words should be used in the list. The algorithm works as such:\n",
    "# 1) Create a sentence using a \"naive\" prediction which simply uses the first word in the array\n",
    "# 2) Iterate through the phonemes in the sentence, if it only has one option than use it, else move to step 3\n",
    "# 3) Use BERT a bi-directional NLP package to create a similarity matrix of most likely words in the sentence\n",
    "# 4) Join array of similar words with possible words associated with the phoneme\n",
    "# 5) If the array is empty default to the first word, else pick the highest ranked word\n",
    "# 6) Add the word to the list and move to the next phoneme \n",
    "\n",
    "def getenglish(haiku):\n",
    "    english_haiku = []\n",
    "    # use getwords to get phonemes\n",
    "    phonemes = getwords(haiku)\n",
    "    # get 2D array of lists of possibilities for each word\n",
    "    wordsarray = phonemes.apply(lambda x: idict[x])\n",
    "    # Get a baseline to use predictions off of by taking the first word for each list\n",
    "    baseline ='[CLS] '\n",
    "    baseline = baseline + ' '.join(wordsarray.apply(lambda x: x[0])) # Use first word\n",
    "    baseline = baseline + ' [SEP]'\n",
    "    for index, word in enumerate(wordsarray):\n",
    "        \n",
    "        if len(word[0])>1:\n",
    "            try:\n",
    "                # predict best word to use for each phoeneme\n",
    "                sentence = re.sub(word[0],'[MASK]',baseline)\n",
    "\n",
    "                tokenized_text = tokenizer.tokenize(sentence)\n",
    "                indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "                # Create the segments tensors.\n",
    "                segments_ids = [0] * len(indexed_tokens)\n",
    "\n",
    "                # Convert inputs to PyTorch tensors\n",
    "                tokens_tensor = torch.tensor([indexed_tokens])\n",
    "                segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "                # Predict all tokens\n",
    "                with torch.no_grad():\n",
    "                    predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "                predicted_words = pd.DataFrame(predictions[0,index])\n",
    "                predicted_words['Word'] = [tokenizer.convert_ids_to_tokens([x])[0] for x in range(len(predicted_words))]\n",
    "                # Create a word dataframe to merge with\n",
    "                wordlist = pd.DataFrame(word,columns=['Word'])\n",
    "                best_word = pd.merge(predicted_words,wordlist, on ='Word').sort_values(0, ascending= False).loc[0,'Word']\n",
    "\n",
    "            # If BERT is unable to predict word then use the first one\n",
    "            except:\n",
    "                best_word=word[0]\n",
    "\n",
    "        else:\n",
    "            best_word=word[0]\n",
    "            \n",
    "        english_haiku.append(best_word)\n",
    "    return english_haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'skein',\n",
       " 'of',\n",
       " 'birds',\n",
       " 'twines',\n",
       " 'across',\n",
       " 'the',\n",
       " 'sky',\n",
       " 'the',\n",
       " 'northbound',\n",
       " 'train',\n",
       " 'departs']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getenglish(haikus_transformed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Haikus w/ Correct Syllable Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Syllables \n",
    "df_syll = pd.DataFrame(zip(haikus_transformed,\n",
    "    haikus_transformed.apply(lambda x: sum(letter.isdigit() for letter in x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1816 172055\n"
     ]
    }
   ],
   "source": [
    "# We need poems with 17 syllables (5 + 7 + 5)\n",
    "df_17_syll = df_syll[df_syll[1]==17]\n",
    "print(len(df_17_syll),len(arraytotext(df_17_syll[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extremley dissapointing result. 1816 haikus wont even scratch the surface of the amount of text needed for an RNN to learn a 5-7-5 syllable structure. \n",
    "\n",
    "However, the goal of the project is to create an RNN model that can understand syllables and learn enlgish as phoenemes. Although some context will be loss, these goals can be met by treating all haikus as one single text and then split into several 5-7-5 haikus individually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2449369"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return Haikus Back to Text\n",
    "haiku_text = arraytotext(haikus_transformed)\n",
    "# Get all words\n",
    "words = getwords(haiku_text)\n",
    "# Get syllables for each word\n",
    "syllables = words.apply(lambda x: sum(letter.isdigit() for letter in x))\n",
    "# zip lists together\n",
    "syll_list = [*zip(words,syllables)]     \n",
    "len(syll_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert list of words into 5-7-5 snippets\n",
    "haikus = []\n",
    "while len(syll_list) > 17:\n",
    "    cum = 0\n",
    "    haiku = []\n",
    "    line = 1\n",
    "    for i,tup in enumerate(syll_list):\n",
    "        cum = cum + tup[1]\n",
    "        if line == 1:\n",
    "            if cum < 5:      \n",
    "                haiku.append(syll_list.pop(i)[0])\n",
    "\n",
    "            elif cum==5:\n",
    "                haiku.append(syll_list.pop(i)[0])\n",
    "                haiku.append('\\n')\n",
    "                line = 2\n",
    "                \n",
    "            # if line 1 has word with a 5 syllable remove it\n",
    "            elif tup[1] > 5:\n",
    "                del syll_list[i]\n",
    "                cum = cum - tup[1]\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                cum = cum - tup[1]\n",
    "                continue\n",
    "                \n",
    "        elif line == 2:\n",
    "            if cum < 12:\n",
    "                haiku.append(syll_list.pop(i)[0])\n",
    "\n",
    "            elif cum == 12:\n",
    "                #print('Second Break:' + tup[0]+ 'Cummulative' + str(cum))\n",
    "                haiku.append(syll_list.pop(i)[0])\n",
    "                haiku.append('\\n')\n",
    "                line = 3\n",
    "            \n",
    "            # if line 1 has word with a 7 syllable remove it\n",
    "            elif tup[1] > 6:\n",
    "                del syll_list[i]\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                cum = cum - tup[1]\n",
    "                continue\n",
    "                \n",
    "        elif line == 3:\n",
    "            if cum < 17:\n",
    "                #print('Third:' + tup[0]+ 'Cummulative' + str(cum))\n",
    "                haiku.append(syll_list.pop(i)[0])\n",
    "                \n",
    "            elif cum == 17:\n",
    "                haiku.append(syll_list.pop(i)[0])\n",
    "                haikus.append(haiku)\n",
    "                #print('Haiku Ends, Words to go:' + str(len(syll_list)))\n",
    "                break\n",
    "             # if line 1 has word with a 5 syllable remove it\n",
    "            elif tup[1] > 4:\n",
    "                del syll_list[i]\n",
    "                cum = cum - tup[1]\n",
    "                continue\n",
    "                    \n",
    "            else:\n",
    "                cum = cum - tup[1]\n",
    "                continue      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DIH0STRAH0KSHAH0N',\n",
       " 'OW0SHAH0N',\n",
       " '\\n',\n",
       " 'DEH0SAH0LAH0T',\n",
       " 'WAA0CH',\n",
       " 'RAY0T',\n",
       " 'JHEH0NTLIY0',\n",
       " '\\n',\n",
       " 'DHAH0',\n",
       " 'AA0R',\n",
       " 'AH0BAE0NDAH0ND']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haikus[len(haikus)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['destruction',\n",
       " 'ocean',\n",
       " 'desolate',\n",
       " 'watch',\n",
       " 'right',\n",
       " 'gently',\n",
       " 'the',\n",
       " 'r',\n",
       " 'abandoned']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getenglish(arraytotext(haikus[len(haikus)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "haiku_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insertbreaks(haiku):\n",
    "#     words = getwords(haiku)\n",
    "#     word_syllables = words.apply(lambda x: sum(letter.isdigit() for letter in x))\n",
    "#     #Get cumulative\n",
    "#     cum_syllables = np.cumsum(word_syllables)\n",
    "#     # Add double line break to signify end of haiku\n",
    "#     #words[len(words)-1] = words[len(words)-1]+'\\n\\n'\n",
    "#     first_break = 0\n",
    "#     for i,num in enumerate(cum_syllables):\n",
    "#         if first_break == 0:\n",
    "#             if(num >= 5):\n",
    "#                 words[i] = words[i]+'\\n'\n",
    "#                 first_break = 1\n",
    "#         else:\n",
    "#             if(num >= 12):\n",
    "#                 words[i] = words[i]+'\\n'\n",
    "#                 return arraytotext(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_haikus = df_17_syll[0].apply(insertbreaks)\n",
    "# Remove space after \\n that was crated it is not needed\n",
    "formatted_haikus = formatted_haikus.apply(lambda x: x.replace('\\n ',\"\\n\")).reset_index(drop=True)\n",
    "formatted_text = arraytotext(formatted_haikus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Haikus into a DataFrame\n",
    "df_haikus = pd.DataFrame(formatted_haikus)\n",
    "df_haikus.columns = ['text']\n",
    "# Get Unique Letters\n",
    "vocab = sorted(set(formatted_text))\n",
    "print(vocab)\n",
    "print('Unique Characters: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def encodehaikus(haiku):\n",
    "    encoded_haiku = np.array([char2idx[c] for c in haiku])\n",
    "    return encoded_haiku\n",
    "encoded_haikus = formatted_haikus.apply(encodehaikus)\n",
    "df_haikus['encoded'] = encoded_haikus\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{}\\n v ---- characters mapped to int ---- v \\n{}'\\\n",
    "       .format(repr(df_haikus.loc[0,'text']), df_haikus.loc[0,'encoded']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training examples and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences with 0s so they are all the same length\n",
    "\n",
    "# Get character lengths of each haiku \n",
    "df_haikus['length'] = df_haikus['encoded'].apply(lambda x:len(x))\n",
    "max_length = df_haikus['length'].max()\n",
    "\n",
    "def getpadded(row):\n",
    "    leng = row['length']\n",
    "    zeros = np.zeros((max_length-leng), dtype=np.int32)\n",
    "    padded = np.append(row['encoded'],zeros)\n",
    "    return padded\n",
    "\n",
    "df_haikus['padded'] = df_haikus.apply(getpadded,axis=1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((df_haikus['input_text'],df_haikus['target_text']))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 958,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 962,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 963,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 965,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 967,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 968,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 969,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('haiku_v1.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = create_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights('haiku_v1.h5')\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 113\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
