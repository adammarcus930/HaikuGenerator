{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from nltk.corpus import cmudict\n",
    "import copy\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "#%load_ext line_profiler\n",
    "cmudict = cmudict.dict()\n",
    "# Load pre-trained BERT model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Functions to Assist in Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into words\n",
    "def getwords(text):\n",
    "    words = pd.Series(re.findall(r\"[\\w']+\", text))\n",
    "    return words\n",
    "# Split text into individual haikus\n",
    "def gethaikus(text):\n",
    "    haikus = pd.Series(text.split(\"\\n\\n\"))\n",
    "    return haikus\n",
    "# Convert an array of words to a single string\n",
    "def arraytotext(arr):\n",
    "    text = ' '.join(arr)\n",
    "    return text\n",
    "# Find words that are not located in the cmudict phoenetic dictionary\n",
    "def getunknowns(words):\n",
    "    unknown_words = np.array([word for word in words if word.lower() not in cmudict.keys()])\n",
    "    return unknown_words\n",
    "# Gets frequency of word use in a given list of words\n",
    "def wordcount(unknown_words):\n",
    "    prob_word_freq = pd.Series(Counter(unknown_words)).sort_values(ascending=False)\n",
    "    return prob_word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Haikus in the Same Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_root = '../Haikus'\n",
    "\n",
    "# Source 1\n",
    "text = open(source_root+'/haikuzao.txt', 'r').read()\n",
    "text = text.lower()\n",
    "haikus = gethaikus(text)\n",
    "# Source 2\n",
    "gutenberg = pd.read_csv(source_root+'/gutenberg.csv')\n",
    "haikus = haikus.append(pd.Series(gutenberg['haiku']).apply(lambda x: x.lower()))\n",
    "# Source 3\n",
    "modern_renaissance = pd.read_csv(source_root+'/modern_renaissance.csv')\n",
    "# make lower case and ensure that the new line notation is the same\n",
    "modern_renaissance = pd.Series(modern_renaissance['content']).apply(lambda x: x.lower().replace(\"\\r\\n\", \"\\n\"))\n",
    "haikus = haikus.append(modern_renaissance)\n",
    "# Source 4\n",
    "sballas = pd.read_csv(source_root+'/sballas8.csv', header=None)\n",
    "haikus = haikus.append(pd.Series(sballas[0]))\n",
    "# Source 5\n",
    "temps = pd.read_csv(source_root+'/tempslibres.csv', encoding = \"ISO-8859-1\")\n",
    "# Only English\n",
    "temps = temps[temps['lang']=='en']\n",
    "# make lower case and ensure that the new line notation is the same\n",
    "haikus = haikus.append(pd.Series(temps['haiku']).apply(lambda x: x.lower().replace(\"\\r\\n\", \"\\n\")))\n",
    "# Source 6\n",
    "hjson = pd.read_json(source_root+'/unim_poem.json')\n",
    "haikus = haikus.append(pd.Series(hjson['poem'])).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "youll         466\n",
       "didnt         418\n",
       "iii           417\n",
       "_             378\n",
       "nbsp          358\n",
       "doesnt        279\n",
       "couldnt       258\n",
       "aught         248\n",
       "isnt          235\n",
       "colours       185\n",
       "heavn         184\n",
       "wasnt         181\n",
       "theyll        181\n",
       "aint          173\n",
       "twixt         168\n",
       "wouldnt       144\n",
       "whateer       123\n",
       "beauteous     121\n",
       "splendour     120\n",
       "gie           116\n",
       "loveliness    113\n",
       "evry          112\n",
       "theyd         100\n",
       "instr          96\n",
       "whereer        94\n",
       "wretch         91\n",
       "lovd           89\n",
       "theyve         87\n",
       "neighbours     87\n",
       "nokomis        86\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words from haikus and determine whih ones do not exist in \n",
    "text = arraytotext(haikus)\n",
    "unknown_words = getunknowns(getwords(text))\n",
    "wordcount(unknown_words).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cuisses', 'lespace', 'morceau', ..., 'doesnt', 'didnt', 'wifi'],\n",
       "      dtype='<U127')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted characters\n",
    "def cleanwords(text):    \n",
    "    #Clean dashes\n",
    "    text = text.replace('-',\" \")\n",
    "    #Clean apostrophe\n",
    "    text = text.replace('\\'',\"\")\n",
    "    # Clean numbers\n",
    "    words = getwords(text)\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            text = re.sub(rf'\\b{word}\\b',num2words(word),text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython/7.5.0/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Clean Haikus\n",
    "haikus = haikus.apply(cleanwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Words to Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert plain text to Phoenetic\n",
    "def getphoneme(word):\n",
    "    phoneme = ''.join(cmudict[word][0]) # always use first pronuciations at index 0\n",
    "    return phoneme\n",
    "\n",
    "# Finds unknown words that can be split into two words\n",
    "def findwordsplits(unknown_words):\n",
    "    splitwords = {}\n",
    "    for word in unknown_words:\n",
    "        wordlength = len(word)\n",
    "        for i in range(wordlength):\n",
    "            split1 = word[0:i+1]\n",
    "            split2 = word[i+1:wordlength]\n",
    "\n",
    "            if split1 in cmudict.keys():\n",
    "                if split2 in cmudict.keys():\n",
    "                    splitwords[word] = [split1,split2]      \n",
    "    return splitwords\n",
    "splitwords = findwordsplits(set(unknown_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['oo', 'o'],\n",
       " ['atter', 'y'],\n",
       " ['singul', 'o'],\n",
       " ['git', 'e'],\n",
       " ['pianger', 'e'],\n",
       " ['vii', 'i'],\n",
       " ['e', 'letto'],\n",
       " ['hetend', 'e'],\n",
       " ['sublunar', 'y'],\n",
       " ['indi', 'o'],\n",
       " ['wilf', 'u'],\n",
       " ['voler', 'i'],\n",
       " ['secg', 'e'],\n",
       " ['tov', 'e'],\n",
       " ['yes', 'e'],\n",
       " ['villan', 'y'],\n",
       " ['u', 'ensis'],\n",
       " ['y', 'cleane'],\n",
       " ['bursat', 'i'],\n",
       " ['u', 'omo']]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect what words have single letters as vowels to determine appropriate sounds\n",
    "vowels = ['a', 'e', 'i','o', 'u', 'y']\n",
    "splitvowlels = []\n",
    "for i in splitwords:\n",
    "    if (len(splitwords[i][0]) ==1 or len(splitwords[i][1]) ==1):\n",
    "        if(splitwords[i][0] in vowels or splitwords[i][1] in vowels):\n",
    "            splitvowlels.append(splitwords[i])\n",
    "splitvowlels[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle words with single letters separately\n",
    "# Note: this may not always be accruacte but should approximate well enough for the intended goal\n",
    "def convertsplitwords(splitwords):\n",
    "    worddict = copy.deepcopy(splitwords)\n",
    "    special_cases = { \n",
    "        'a':'AH0',\n",
    "        'i': 'IY0',\n",
    "        'o': 'OW0',\n",
    "        'u' : 'UW0',\n",
    "        'y':'IY0',\n",
    "        's': 'Z'\n",
    "    }\n",
    "    \n",
    "    for i in worddict:\n",
    "        \n",
    "        firstword = worddict[i][0]\n",
    "        secondword = worddict[i][1]\n",
    "\n",
    "        # If the first word is a single letter then make it all caps\n",
    "        if (len(firstword)==1):\n",
    "            firstword = firstword.upper()\n",
    "            \n",
    "        # Otherwise use CMUDICT to convert\n",
    "        else:\n",
    "            firstword = getphoneme(firstword)\n",
    "\n",
    "        # If the second word is a single letter then apply the special cases where applicable\n",
    "        if (len(secondword)==1):\n",
    "            \n",
    "            # If letter is a special case replace it\n",
    "            if secondword in special_cases.keys():\n",
    "                secondword = pd.Series(worddict[i]).replace(special_cases)[1]\n",
    "                \n",
    "            # Otherwise make it all caps\n",
    "            else:\n",
    "                secondword = secondword.upper()\n",
    "                \n",
    "       # Otherwise use CMUDICT to convert\n",
    "        else:\n",
    "            secondword = getphoneme(secondword)\n",
    "            \n",
    "        # added nested list to match format of CMUDICT\n",
    "        worddict[i] = [[firstword,secondword]]   \n",
    "        \n",
    "    return worddict\n",
    "\n",
    "# takes in haikus and separates haikus that have words not in the CMUDICT\n",
    "def splithaikus(haikus):\n",
    "    \n",
    "    bad_haikus = []\n",
    "    good_haikus = []\n",
    "    \n",
    "    for haiku in haikus:\n",
    "        words = getwords(haiku)\n",
    "        if all(word in cmudict.keys() for word in words):\n",
    "            good_haikus.append(haiku)\n",
    "        else:\n",
    "            bad_haikus.append(haiku)\n",
    "                        \n",
    "    return bad_haikus, good_haikus\n",
    "\n",
    "# Takes in a haiku and transforms it into the equivalent phoneme version\n",
    "def haikutransform(haiku):\n",
    "    words = getwords(haiku)\n",
    "    try:\n",
    "        phonemes = [* map(getphoneme,words)]\n",
    "        phonemes = ' '.join(phonemes)\n",
    "    except:\n",
    "        raise ValueError('A word in the Haiku was not in the CMUDICT.' \\\n",
    "        ' Make sure only valid haikus are used for this function.')\n",
    "        return\n",
    "        \n",
    "    return phonemes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/ipython/7.5.0/libexec/vendor/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# add new word phoenetics to cmudict\n",
    "cmudict = {**cmudict ,**convertsplitwords(splitwords)}\n",
    "# split haikus into a usuable set and a set that can be further inspected for transformations\n",
    "bad_haikus, valid_haikus = splithaikus(haikus)\n",
    "# transform the good haikus into phonemes\n",
    "haikus_transformed = pd.Series(map(haikutransform,valid_haikus))\n",
    "# Convert all syllables to 0\n",
    "haikus_transformed = pd.Series(map(convertsyllables,haikus_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if we didnt lose any haikus\n",
    "len(bad_haikus) + len(haikus_transformed) == len(haikus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Function to Convert Back to Regular English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the cmudict so that we can transform from a phoneme to english\n",
    "def invertdictionary(cmudict):\n",
    "    \n",
    "    idict = {}\n",
    "    \n",
    "    for word in cmudict:\n",
    "        # Create a list to hold all of the possible words associated with a phoneme\n",
    "        p_list = []\n",
    "        \n",
    "        phoneme = ''.join(cmudict[word][0]) # always use first phoneme for a word\n",
    "        phoneme = convertsyllables(phoneme)\n",
    "        \n",
    "        # if the phoneme already exists add it to that list\n",
    "        if phoneme in idict.keys():\n",
    "            p_list = idict[phoneme]\n",
    "            p_list.append(word)\n",
    "            \n",
    "        # Otherwise create a new list\n",
    "        else:\n",
    "            p_list.append(word)\n",
    "           \n",
    "        idict[phoneme] = p_list\n",
    "    return idict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Inverted Phoneme Dictionary\n",
    "idict = invertdictionary(copy.deepcopy(cmudict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllablecount(phoneme):\n",
    "    count = 0\n",
    "    for letter in phoneme:\n",
    "        if letter.isdigit():\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "def getwordarray(phoneme):\n",
    "    try:\n",
    "        words = idict[phoneme]\n",
    "    # if there are no words in the dictionary then create a flag to handle later\n",
    "    except:\n",
    "        print(\"BadPhoneme\")\n",
    "        syllables = syllablecount(phoneme)\n",
    "        words = ['',syllables]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes in haikus and predicts what words should be used in the list. The algorithm works as such:\n",
    "# 1) Create a sentence using a \"naive\" prediction which simply uses the first word in the array\n",
    "# 2) Iterate through the phonemes in the sentence, if it only has one option than use it, else move to step 3\n",
    "# 3) Use BERT a bi-directional NLP package to create a similarity matrix of most likely words in the sentence\n",
    "# 4) Join array of similar words with possible words associated with the phoneme\n",
    "# 5) If the array is empty default to the first word, else pick the highest ranked word\n",
    "# 6) Add the word to the list and move to the next phoneme \n",
    "\n",
    "def getenglish(haiku):\n",
    "    english_haiku = []\n",
    "    # use getwords to get phonemes\n",
    "    phonemes = getwords(haiku)\n",
    "    # get 2D array of lists of possibilities for each word\n",
    "    wordsarray = phonemes.apply(getwordarray)\n",
    "    # Get a baseline to use predictions off of by taking the first word for each list\n",
    "    baseline ='[CLS] '\n",
    "    baseline = baseline + ' '.join(wordsarray.apply(lambda x: x[0])) # Use first word\n",
    "    baseline = baseline + ' [SEP]'\n",
    "    for index, word in enumerate(wordsarray):\n",
    "        if len(word)>1 :\n",
    "            try:                \n",
    "                    #Replace word with mask\n",
    "                sentence = re.sub(word[0],'[MASK]',baseline)\n",
    "\n",
    "                tokenized_text = tokenizer.tokenize(sentence)\n",
    "                indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "                # Create the segments tensors.\n",
    "                segments_ids = [0] * len(indexed_tokens)\n",
    "\n",
    "                # Convert inputs to PyTorch tensors\n",
    "                tokens_tensor = torch.tensor([indexed_tokens])\n",
    "                segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "                # Predict all tokens\n",
    "                with torch.no_grad():\n",
    "                    predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "                predicted_words = pd.DataFrame(predictions[0,index])\n",
    "                predicted_words['Word'] = [tokenizer.convert_ids_to_tokens([x])[0] for x in range(len(predicted_words))]\n",
    "                # If its an actual word then merget the predicted words with the word array and grab the\n",
    "                # highest ranked word\n",
    "                if type(word[1]) != int :\n",
    "                    # Create a word dataframe to merge with\n",
    "                    wordlist = pd.DataFrame(word,columns=['Word'])\n",
    "                    best_word = pd.merge(predicted_words,wordlist, on ='Word').sort_values(0, ascending= False).loc[0,'Word']\n",
    "\n",
    "                # If the word is a number then that means there was are no english words associated with the phoneme\n",
    "                # So instead use the highest ranked word with the same syllable count\n",
    "                else:\n",
    "                    # The second value is the syllable count created by the get word array function\n",
    "                    syllabes = word[1]\n",
    "                    #Sort Predicted words\n",
    "                    predicted_words.sort_values(0, ascending= False, inplace = True)\n",
    "                    #Loop through and find the first word with the same syllable count\n",
    "                    for word in predicted_words['Word']:\n",
    "                        # Try to get syllable count of the word\n",
    "                        try:\n",
    "                            pred_syllable = syllablecount(getphoneme(word))\n",
    "                            if pred_syllable == syllabes:\n",
    "                                best_word = word\n",
    "                                break\n",
    "                        # If the word doesnt exist then skip that word\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                    # If the word still wasnt found then grab the best word\n",
    "                    # Noted: This will create an error in the syllable co\n",
    "                    if type(word[0]) == int:\n",
    "                        best_word = predicted_words.loc[0,'Word']\n",
    "                # If BERT is unable to predict word then use the first word\n",
    "            except:\n",
    "                best_word=word[0]\n",
    "                \n",
    "        else:\n",
    "            best_word=word[0]\n",
    "            \n",
    "        english_haiku.append(best_word)\n",
    "    return english_haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a skein of birds twines across the sky the northbound train departs\n",
      "a skein of birds\n",
      "twines across the sky\n",
      "the northbound train departs\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the function correctly tranforms phonemes back to english correctly\n",
    "print(arraytotext(getenglish(haikus_transformed[0])))\n",
    "print(valid_haikus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Haikus w/ Correct Syllable Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Syllables \n",
    "df_syll = pd.DataFrame(zip(haikus_transformed,\n",
    "    haikus_transformed.apply(syllablecount)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1816 172055\n"
     ]
    }
   ],
   "source": [
    "# We need poems with 17 syllables (5 + 7 + 5)\n",
    "df_17_syll = df_syll[df_syll[1]==17]\n",
    "print(len(df_17_syll),len(arraytotext(df_17_syll[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extremley dissapointing result. 1890 haikus wont even scratch the surface of the amount of text needed for an RNN to learn a 5-7-5 syllable structure. \n",
    "\n",
    "However, the goal of the project is to create an RNN model that can understand syllables and learn enlgish as phoenemes. Although some context will be loss, these goals can be met by treating all haikus as one single text and then split into several 17 syllables poems. If the model can learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2449369"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return Haikus Back to Text\n",
    "haiku_text = arraytotext(haikus_transformed)\n",
    "# Get all words\n",
    "words = getwords(haiku_text)\n",
    "# Get syllables for each word\n",
    "syllables = words.apply(syllablecount)\n",
    "# zip lists together\n",
    "syll_list = [*zip(words,syllables)]     \n",
    "len(syll_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of words into 5-7-5 snippets\n",
    "haikus17 = []\n",
    "while len(syll_list) > 17:\n",
    "    cum = 0\n",
    "    haiku = []\n",
    "    \n",
    "    for i,tup in enumerate(syll_list):\n",
    "        cum = cum + tup[1]\n",
    "        if cum < 17:      \n",
    "            haiku.append(syll_list.pop(i)[0])\n",
    "\n",
    "        elif cum == 17:\n",
    "            haiku.append(syll_list.pop(i)[0])\n",
    "            haiku.append('\\n')\n",
    "            haikus17.append(haiku)\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            cum = cum - tup[1]\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AH0',\n",
       " 'STIH0L',\n",
       " 'BAY0',\n",
       " 'DAA0NGKIY0Z',\n",
       " 'PEY0N',\n",
       " 'AA0N',\n",
       " 'PAE0N',\n",
       " 'GAA0D',\n",
       " 'DAA0RKAH0NIH0NG',\n",
       " 'AH0',\n",
       " 'AH0V',\n",
       " 'BRAY0TLIY0',\n",
       " 'MUW0N',\n",
       " '\\n']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if Haikus are 17 syllables structure\n",
    "haikus17[len(haikus17)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'still',\n",
       " 'by',\n",
       " 'donkeys',\n",
       " 'pain',\n",
       " 'on',\n",
       " 'pan',\n",
       " 'god',\n",
       " 'darkening',\n",
       " 'a',\n",
       " 'of',\n",
       " 'brightly',\n",
       " 'moon']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getenglish(arraytotext(haikus17[len(haikus17)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text looks good; Output it to CSV so transformations do not have to be ran again\n",
    "haiku_series = pd.Series(haikus17)\n",
    "haiku_series = haiku_series.apply(arraytotext)\n",
    "haiku_series.to_csv('../Haikus/PhonemeHaikusStructured.csv',index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185880"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(haiku_series)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
