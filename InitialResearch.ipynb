{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "from nltk.corpus import cmudict\n",
    "import copy\n",
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "cmudict = cmudict.dict()\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Functions to Assist in Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text into words\n",
    "def getwords(text):\n",
    "    words = pd.Series(re.findall(r\"[\\w']+\", text))\n",
    "    return words\n",
    "\n",
    "\n",
    "# Split text into individual haikus\n",
    "def gethaikus(text):\n",
    "    haikus = pd.Series(text.split(\"\\n\\n\"))\n",
    "    return haikus\n",
    "\n",
    "\n",
    "# Convert an array of words to a single string\n",
    "def arraytotext(arr):\n",
    "    text = \" \".join(arr)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Find words that are not located in the cmudict phoenetic dictionary\n",
    "def getunknowns(words):\n",
    "    unknown_words = np.array(\n",
    "        [word for word in words if word.lower() not in cmudict.keys()]\n",
    "    )\n",
    "    return unknown_words\n",
    "\n",
    "\n",
    "# Gets frequency of word use in a given list of words\n",
    "def wordcount(unknown_words):\n",
    "    prob_word_freq = pd.Series(Counter(unknown_words)).sort_values(\n",
    "        ascending=False\n",
    "    )\n",
    "    return prob_word_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import All Haikus in the Same Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_root = \"Haikus\"\n",
    "\n",
    "# Source 1\n",
    "text = open(source_root + \"/haikuzao.txt\", \"r\").read()\n",
    "text = text.lower()\n",
    "haikus = gethaikus(text)\n",
    "# Source 2\n",
    "gutenberg = pd.read_csv(source_root + \"/gutenberg.csv\")\n",
    "haikus = haikus.append(\n",
    "    pd.Series(gutenberg[\"haiku\"]).apply(lambda x: x.lower())\n",
    ")\n",
    "# Source 3\n",
    "modern_renaissance = pd.read_csv(source_root + \"/modern_renaissance.csv\")\n",
    "# make lower case and ensure that the new line notation is the same\n",
    "modern_renaissance = pd.Series(modern_renaissance[\"content\"]).apply(\n",
    "    lambda x: x.lower().replace(\"\\r\\n\", \"\\n\")\n",
    ")\n",
    "haikus = haikus.append(modern_renaissance)\n",
    "# Source 4\n",
    "sballas = pd.read_csv(source_root + \"/sballas8.csv\", header=None)\n",
    "haikus = haikus.append(pd.Series(sballas[0]))\n",
    "# Source 5\n",
    "temps = pd.read_csv(source_root + \"/tempslibres.csv\", encoding=\"ISO-8859-1\")\n",
    "# Only English\n",
    "temps = temps[temps[\"lang\"] == \"en\"]\n",
    "# make lower case and ensure that the new line notation is the same\n",
    "haikus = haikus.append(\n",
    "    pd.Series(temps[\"haiku\"]).apply(lambda x: x.lower().replace(\"\\r\\n\", \"\\n\"))\n",
    ")\n",
    "# Source 6\n",
    "hjson = pd.read_json(source_root + \"/unim_poem.json\")\n",
    "haikus = haikus.append(pd.Series(hjson[\"poem\"])).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "an'       2419\n",
       "o'er      2041\n",
       "sg        1421\n",
       "wi'        879\n",
       "sc         646\n",
       "'          614\n",
       "pl         591\n",
       "a'         564\n",
       "acc        533\n",
       "'t         500\n",
       "ii         479\n",
       "iii        417\n",
       "th'        403\n",
       "sae        379\n",
       "_          378\n",
       "nbsp       358\n",
       "iv         292\n",
       "honour     277\n",
       "nought     275\n",
       "tho'       272\n",
       "nae        264\n",
       "thro'      259\n",
       "aught      248\n",
       "hae        246\n",
       "e'er       243\n",
       "beheld     223\n",
       "frae       215\n",
       "e'en       213\n",
       "canst      211\n",
       "quoth      208\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get words from haikus and determine whih ones do not exist in\n",
    "text = arraytotext(haikus)\n",
    "unknown_words = getunknowns(getwords(text))\n",
    "wordcount(unknown_words).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unwanted characters\n",
    "def cleanwords(text):\n",
    "    # Clean dashes\n",
    "    text = text.replace(\"-\", \" \")\n",
    "    # Clean apostrophe\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    # Clean numbers\n",
    "    words = getwords(text)\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            text = re.sub(rf\"\\b{word}\\b\", num2words(word), text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-0b9d567929d4>:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  words = pd.Series(re.findall(r\"[\\w']+\", text))\n"
     ]
    }
   ],
   "source": [
    "# Clean Haikus\n",
    "haikus = haikus.apply(cleanwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Words to Phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert plain text to Phoenetic\n",
    "def getphoneme(word):\n",
    "    phoneme = \"\".join(\n",
    "        cmudict[word][0]\n",
    "    )  # always use first pronuciations at index 0\n",
    "    return phoneme\n",
    "\n",
    "\n",
    "# Finds unknown words that can be split into two words\n",
    "def findwordsplits(unknown_words):\n",
    "    splitwords = {}\n",
    "    for word in unknown_words:\n",
    "        wordlength = len(word)\n",
    "        for i in range(wordlength):\n",
    "            split1 = word[0 : i + 1]\n",
    "            split2 = word[i + 1 : wordlength]\n",
    "\n",
    "            if split1 in cmudict.keys():\n",
    "                if split2 in cmudict.keys():\n",
    "                    splitwords[word] = [split1, split2]\n",
    "    return splitwords\n",
    "\n",
    "\n",
    "splitwords = findwordsplits(set(unknown_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['som', 'u'],\n",
       " ['lather', 'y'],\n",
       " ['mull', 'a'],\n",
       " ['tant', 'a'],\n",
       " ['pris', 'e'],\n",
       " ['colebrook', 'e'],\n",
       " ['ruin', 'a'],\n",
       " ['mint', 'y'],\n",
       " ['dice', 'a'],\n",
       " ['os', 'a'],\n",
       " ['omar', 'e'],\n",
       " ['porta', 'i'],\n",
       " ['pagani', 'a'],\n",
       " ['quail', 'y'],\n",
       " ['fenn', 'y'],\n",
       " ['cole', 'i'],\n",
       " ['discern', 'e'],\n",
       " ['seder', 'e'],\n",
       " ['tumult', 'o'],\n",
       " ['farr', 'e']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect what words have single letters as vowels\n",
    "# to determine appropriate sounds\n",
    "vowels = [\"a\", \"e\", \"i\", \"o\", \"u\", \"y\"]\n",
    "splitvowlels = []\n",
    "for i in splitwords:\n",
    "    if len(splitwords[i][0]) == 1 or len(splitwords[i][1]) == 1:\n",
    "        if splitwords[i][0] in vowels or splitwords[i][1] in vowels:\n",
    "            splitvowlels.append(splitwords[i])\n",
    "splitvowlels[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle words with single letters separately\n",
    "# Note: this may not always be accruacte\n",
    "# but should approximate well enough for the intended goal\n",
    "def convertsplitwords(splitwords):\n",
    "    worddict = copy.deepcopy(splitwords)\n",
    "    special_cases = {\n",
    "        \"a\": \"AH0\",\n",
    "        \"i\": \"IY0\",\n",
    "        \"o\": \"OW0\",\n",
    "        \"u\": \"UW0\",\n",
    "        \"y\": \"IY0\",\n",
    "        \"s\": \"Z\",\n",
    "    }\n",
    "\n",
    "    for i in worddict:\n",
    "\n",
    "        firstword = worddict[i][0]\n",
    "        secondword = worddict[i][1]\n",
    "\n",
    "        # If the first word is a single letter then make it all caps\n",
    "        if len(firstword) == 1:\n",
    "            firstword = firstword.upper()\n",
    "\n",
    "        # Otherwise use CMUDICT to convert\n",
    "        else:\n",
    "            firstword = getphoneme(firstword)\n",
    "\n",
    "        # If second word is a single letter apply the special cases\n",
    "        if len(secondword) == 1:\n",
    "\n",
    "            # If letter is a special case replace it\n",
    "            if secondword in special_cases.keys():\n",
    "                secondword = pd.Series(worddict[i]).replace(special_cases)[1]\n",
    "\n",
    "            # Otherwise make it all caps\n",
    "            else:\n",
    "                secondword = secondword.upper()\n",
    "\n",
    "        # Otherwise use CMUDICT to convert\n",
    "        else:\n",
    "            secondword = getphoneme(secondword)\n",
    "\n",
    "        # added nested list to match format of CMUDICT\n",
    "        worddict[i] = [[firstword, secondword]]\n",
    "\n",
    "    return worddict\n",
    "\n",
    "\n",
    "# takes in haikus and separates haikus that have words not in the CMUDICT\n",
    "def splithaikus(haikus):\n",
    "\n",
    "    bad_haikus = []\n",
    "    good_haikus = []\n",
    "\n",
    "    for haiku in haikus:\n",
    "        words = getwords(haiku)\n",
    "        if all(word in cmudict.keys() for word in words):\n",
    "            good_haikus.append(haiku)\n",
    "        else:\n",
    "            bad_haikus.append(haiku)\n",
    "\n",
    "    return bad_haikus, good_haikus\n",
    "\n",
    "\n",
    "# Takes in a haiku and transforms it into the equivalent phoneme version\n",
    "def haikutransform(haiku):\n",
    "    words = getwords(haiku)\n",
    "    try:\n",
    "        phonemes = [*map(getphoneme, words)]\n",
    "        phonemes = \" \".join(phonemes)\n",
    "    except:\n",
    "        raise ValueError(\n",
    "            \"A word in the Haiku was not in the CMUDICT.\"\n",
    "            \" Make sure only valid haikus are used for this function.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    return phonemes\n",
    "\n",
    "\n",
    "# CMUDICT uses numbers (0-2) to denote stress of the syllable.\n",
    "# Although this is something that could be explored later,\n",
    "# it is unnecessary complexity and should be a consistent format.\n",
    "def convertsyllables(text):\n",
    "    text = re.sub(\"(1|2)\", \"0\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-0b9d567929d4>:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  words = pd.Series(re.findall(r\"[\\w']+\", text))\n"
     ]
    }
   ],
   "source": [
    "# add new word phoenetics to cmudict\n",
    "cmudict = {**cmudict, **convertsplitwords(splitwords)}\n",
    "# split haikus into a usuable set and a set that can be further inspected\n",
    "bad_haikus, valid_haikus = splithaikus(haikus)\n",
    "# transform the good haikus into phonemes\n",
    "haikus_transformed = pd.Series(map(haikutransform, valid_haikus))\n",
    "# Convert all syllables to 0\n",
    "haikus_transformed = pd.Series(map(convertsyllables, haikus_transformed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if we didnt lose any haikus\n",
    "len(bad_haikus) + len(haikus_transformed) == len(haikus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Function to Convert Back to Regular English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the cmudict so that we can transform from a phoneme to english\n",
    "def invertdictionary(cmudict):\n",
    "\n",
    "    idict = {}\n",
    "\n",
    "    for word in cmudict:\n",
    "        # Create a list to hold all possible words associated with a phoneme\n",
    "        p_list = []\n",
    "\n",
    "        phoneme = \"\".join(\n",
    "            cmudict[word][0]\n",
    "        )  # always use first phoneme for a word\n",
    "        phoneme = convertsyllables(phoneme)\n",
    "\n",
    "        # if the phoneme already exists add it to that list\n",
    "        if phoneme in idict.keys():\n",
    "            p_list = idict[phoneme]\n",
    "            p_list.append(word)\n",
    "\n",
    "        # Otherwise create a new list\n",
    "        else:\n",
    "            p_list.append(word)\n",
    "\n",
    "        idict[phoneme] = p_list\n",
    "    return idict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Inverted Phoneme Dictionary\n",
    "idict = invertdictionary(copy.deepcopy(cmudict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syllablecount(phoneme):\n",
    "    count = 0\n",
    "    for letter in phoneme:\n",
    "        if letter.isdigit():\n",
    "            count = count + 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def getwordarray(phoneme):\n",
    "    try:\n",
    "        words = idict[phoneme]\n",
    "    # if there are no words in the dictionary then create a flag\n",
    "    except:\n",
    "        print(\"BadPhoneme\")\n",
    "        syllables = syllablecount(phoneme)\n",
    "        words = [\"\", syllables]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes a phoneme list and predicts what words should be used\n",
    "# The algorithm works as such:\n",
    "# 1) Create a \"naive\" prediction which simply uses the first word in the array\n",
    "# 2) Iterate through the phonemes in the sentence,\n",
    "#    if it only has one option than use it, else move to step 3\n",
    "# 3) Use BERT a bi-directional NLP package to create a similarity matrix\n",
    "#    of most likely words in the sentence\n",
    "# 4) Join predicted words with possible words linked to the phoneme\n",
    "# 5) If the array is empty default to the first word,\n",
    "#    else pick the highest ranked word\n",
    "# 6) Add the word to the list and move to the next phoneme\n",
    "def getenglish(haiku):\n",
    "    english_haiku = []\n",
    "    # use getwords to get phonemes\n",
    "    phonemes = getwords(haiku)\n",
    "    # get 2D array of lists of possibilities for each word\n",
    "    wordsarray = phonemes.apply(getwordarray)\n",
    "    # Get a baseline to use predictions off of by with first word\n",
    "    baseline = \"[CLS] \"\n",
    "    baseline = baseline + \" \".join(\n",
    "        wordsarray.apply(lambda x: x[0])\n",
    "    )  # Use first word\n",
    "    baseline = baseline + \" [SEP]\"\n",
    "    for index, word in enumerate(wordsarray):\n",
    "        if len(word) > 1:\n",
    "            try:\n",
    "                # Replace word with mask\n",
    "                sentence = re.sub(word[0], \"[MASK]\", baseline)\n",
    "                tokenized_text = tokenizer.tokenize(sentence)\n",
    "                indexed_tokens = tokenizer.convert_tokens_to_ids(\n",
    "                    tokenized_text\n",
    "                )\n",
    "                segments_ids = [0] * len(indexed_tokens)\n",
    "                tokens_tensor = torch.tensor([indexed_tokens])\n",
    "                segments_tensors = torch.tensor([segments_ids])\n",
    "                # Predict all tokens\n",
    "                with torch.no_grad():\n",
    "                    predictions = model(tokens_tensor, segments_tensors)\n",
    "                predicted_words = pd.DataFrame(predictions[0, index])\n",
    "                predicted_words[\"Word\"] = [\n",
    "                    tokenizer.convert_ids_to_tokens([x])[0]\n",
    "                    for x in range(len(predicted_words))\n",
    "                ]\n",
    "                # If its an actual word then merge predicted words\n",
    "                # with word array and use highest ranked word\n",
    "                if type(word[1]) != int:\n",
    "                    # Create a word dataframe to merge with\n",
    "                    wordlist = pd.DataFrame(word, columns=[\"Word\"])\n",
    "                    best_word = (\n",
    "                        pd.merge(predicted_words, wordlist, on=\"Word\")\n",
    "                        .sort_values(0, ascending=False)\n",
    "                        .loc[0, \"Word\"]\n",
    "                    )\n",
    "                # If the word is a number then there was are no english words\n",
    "                # Instead use highest ranked word with the same syllables\n",
    "                else:\n",
    "                    # The second value is the syllable count\n",
    "                    syllabes = word[1]\n",
    "                    # Sort Predicted words\n",
    "                    predicted_words.sort_values(\n",
    "                        0, ascending=False, inplace=True\n",
    "                    )\n",
    "                    # Find the first word with the same syllable count\n",
    "                    for word in predicted_words[\"Word\"]:\n",
    "                        # Try to get syllable count of the word\n",
    "                        try:\n",
    "                            pred_syllable = syllablecount(getphoneme(word))\n",
    "                            if pred_syllable == syllabes:\n",
    "                                best_word = word\n",
    "                                break\n",
    "                        # If the word doesnt exist then skip that word\n",
    "                        except:\n",
    "                            continue\n",
    "                    # If the word still wasnt found then grab the best word\n",
    "                    # Noted: This will create an error in the syllable co\n",
    "                    if type(word[0]) == int:\n",
    "                        best_word = predicted_words.loc[0, \"Word\"]\n",
    "                # If BERT is unable to predict word then use the first word\n",
    "            except:\n",
    "                best_word = word[0]\n",
    "\n",
    "        else:\n",
    "            best_word = word[0]\n",
    "\n",
    "        english_haiku.append(best_word)\n",
    "    return english_haiku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a skein of birds twines across the sky the northbound train departs\n",
      "a skein of birds\n",
      "twines across the sky\n",
      "the northbound train departs\n"
     ]
    }
   ],
   "source": [
    "# Check to see if function correctly tranforms phonemes back to english\n",
    "print(arraytotext(getenglish(haikus_transformed[0])))\n",
    "print(valid_haikus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Haikus w/ Correct Syllable Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count Syllables\n",
    "df_syll = pd.DataFrame(\n",
    "    zip(haikus_transformed, haikus_transformed.apply(syllablecount))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1816 172055\n"
     ]
    }
   ],
   "source": [
    "# We need poems with 17 syllables (5 + 7 + 5)\n",
    "df_17_syll = df_syll[df_syll[1] == 17]\n",
    "print(len(df_17_syll), len(arraytotext(df_17_syll[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an extremley dissapointing result. 1890 haikus wont even scratch the surface of the amount of text needed for an RNN to learn a 5-7-5 syllable structure. \n",
    "\n",
    "However, the goal of the project is to create an RNN model that can understand syllables and learn enlgish as phoenemes. Although some context will be lost, these goals can be met by treating all haikus as one single text and then split into several 17 syllables poems. If the model can learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2449369"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return Haikus Back to Text\n",
    "haiku_text = arraytotext(haikus_transformed)\n",
    "# Get all words\n",
    "words = getwords(haiku_text)\n",
    "# Get syllables for each word\n",
    "syllables = words.apply(syllablecount)\n",
    "# zip lists together\n",
    "syll_list = [*zip(words, syllables)]\n",
    "len(syll_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list of words into 5-7-5 snippets\n",
    "haikus17 = []\n",
    "while len(syll_list) > 17:\n",
    "    cum = 0\n",
    "    haiku = []\n",
    "    for i, tup in enumerate(syll_list):\n",
    "        cum = cum + tup[1]\n",
    "        if cum < 17:\n",
    "            haiku.append(syll_list.pop(i)[0])\n",
    "        elif cum == 17:\n",
    "            haiku.append(syll_list.pop(i)[0])\n",
    "            haiku.append(\"\\n\")\n",
    "            haikus17.append(haiku)\n",
    "            break\n",
    "        else:\n",
    "            cum = cum - tup[1]\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DHAH0',\n",
       " 'AH0NJHAH0LEY0TS',\n",
       " 'AY0',\n",
       " 'WAA0CH',\n",
       " 'FEH0LT',\n",
       " 'BREY0KS',\n",
       " 'KLOW0S',\n",
       " 'DHAH0',\n",
       " 'DEY0Z',\n",
       " 'AA0R',\n",
       " 'AH0BAE0NDAH0ND',\n",
       " 'IH0N',\n",
       " 'BAY0',\n",
       " '\\n']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if Haikus are 17 syllables structure\n",
    "haikus17[len(haikus17) - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'undulates',\n",
       " 'i',\n",
       " 'watch',\n",
       " 'felt',\n",
       " 'breaks',\n",
       " 'close',\n",
       " 'the',\n",
       " 'days',\n",
       " 'r',\n",
       " 'abandoned',\n",
       " 'in',\n",
       " 'by']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getenglish(arraytotext(haikus17[len(haikus17) - 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text looks good; Output it to CSV so transformations do not have to be ran again\n",
    "haiku_series = pd.Series(haikus17)\n",
    "haiku_series = haiku_series.apply(arraytotext)\n",
    "haiku_series.to_csv(\n",
    "    \"Haikus/PhonemeHaikusStructured.csv\", index=False, header=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
