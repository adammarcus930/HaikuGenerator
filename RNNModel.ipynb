{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyUX8FRQ5MRD"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "import csv\n",
    "import TextAnalyzer as ta\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GRo8RX4p7cXG",
    "outputId": "24a5525e-c5ae-47eb-b84d-475a3cffadc6"
   },
   "outputs": [],
   "source": [
    "#### ONLY USED FOR GOOGLE COLAB TO DETECT GPU #####\n",
    "# %tensorflow_version 2.x\n",
    "# import tensorflow as tf\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#     raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fZY2MM3l5MRO"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Haikus/PhonemeHaikusStructured.csv')\n",
    "df.columns = ['text']\n",
    "formatter = ta.TextFormat()\n",
    "haiku_text = formatter.arraytotext(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovK19nwM5MRZ"
   },
   "source": [
    "# Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YbVzvS095MRZ",
    "outputId": "8fd9fb79-e2bc-4a25-f759-5f2e038d9a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '0', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "Unique Characters: 29\n"
     ]
    }
   ],
   "source": [
    "# Convert Haikus into a DataFrame\n",
    "df.columns = ['text']\n",
    "# Get Unique Letters\n",
    "vocab = sorted(set(haiku_text))\n",
    "print(vocab)\n",
    "print('Unique Characters: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "qRDg8JgD5MRd",
    "outputId": "6debf975-8b30-4972-a11d-8ca04c94d308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '0' :   2,\n",
      "  'A' :   3,\n",
      "  'B' :   4,\n",
      "  'C' :   5,\n",
      "  'D' :   6,\n",
      "  'E' :   7,\n",
      "  'F' :   8,\n",
      "  'G' :   9,\n",
      "  'H' :  10,\n",
      "  'I' :  11,\n",
      "  'J' :  12,\n",
      "  'K' :  13,\n",
      "  'L' :  14,\n",
      "  'M' :  15,\n",
      "  'N' :  16,\n",
      "  'O' :  17,\n",
      "  'P' :  18,\n",
      "  'Q' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "def encodehaikus(haiku):\n",
    "    encoded_haiku = np.array([char2idx[c] for c in haiku])\n",
    "    return encoded_haiku\n",
    "encoded_haikus = df['text'].apply(encodehaikus)\n",
    "df['encoded'] = encoded_haikus\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "J6h5XdgK5MRh",
    "outputId": "f27519eb-f484-4614-e258-2700e34d36ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'SKEY0N AH0KRAO0S NAO0RTHBAW0ND KAO0RAH0S FAO0R NUW0 GRAW0S MIH0SIH0NG ER0AW0ND DEY0 DHAH0 FAO0R \\n'\n",
      " v ---- characters mapped to int ---- v \n",
      "[21 13  7 27  2 16  1  3 10  2 13 20  3 17  2 21  1 16  3 17  2 20 22 10\n",
      "  4  3 25  2 16  6  1 13  3 17  2 20  3 10  2 21  1  8  3 17  2 20  1 16\n",
      " 23 25  2  1  9 20  3 25  2 21  1 15 11 10  2 21 11 10  2 16  9  1  7 20\n",
      "  2  3 25  2 16  6  1  6  7 27  2  1  6 10  3 10  2  1  8  3 17  2 20  1\n",
      "  0]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{}\\n v ---- characters mapped to int ---- v \\n{}'\\\n",
    "       .format(repr(df.loc[0,'text']), df.loc[0,'encoded']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeQKEAqQ5MRk"
   },
   "source": [
    "# Create training examples and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z3LaB58Z5MRl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# Pad sequences with 0s so they are all the same length\n",
    "\n",
    "# Get character lengths of each haiku \n",
    "df['length'] = df['encoded'].apply(lambda x:len(x))\n",
    "max_length = df['length'].max()\n",
    "\n",
    "def getpadded(row):\n",
    "    leng = row['length']\n",
    "    zeros = np.zeros((max_length-leng), dtype=np.int32)\n",
    "    padded = np.append(row['encoded'],zeros)\n",
    "    return padded\n",
    "\n",
    "df['padded'] = df.apply(getpadded,axis=1)   \n",
    "df['input_text'] = df['padded'].apply(lambda x: x[:-1])\n",
    "df['target_text'] = df['padded'].apply(lambda x: x[1:])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YApdaMiKOoZN",
    "outputId": "380c70ee-3f41-40fb-9bd1-e0e7d0baa248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cp_8LQJI5MRo",
    "outputId": "c132c8a0-82a1-4775-b14e-fc3443689308"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: ((127,), (127,)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((list(df['input_text']),list(df['target_text'])))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "D1bCHw9P5MRr",
    "outputId": "6502d729-0243-483d-cf71-f3d3556aaa79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'SKEY0N AH0KRAO0S NAO0RTHBAW0ND KAO0RAH0S FAO0R NUW0 GRAW0S MIH0SIH0NG ER0AW0ND DEY0 DHAH0 FAO0R \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "Target data: 'KEY0N AH0KRAO0S NAO0RTHBAW0ND KAO0RAH0S FAO0R NUW0 GRAW0S MIH0SIH0NG ER0AW0ND DEY0 DHAH0 FAO0R \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "vepQOhWq5MRu",
    "outputId": "7b1d5bd5-8af6-4623-9683-d1ce91d540c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 21 ('S')\n",
      "  expected output: 13 ('K')\n",
      "Step    1\n",
      "  input: 13 ('K')\n",
      "  expected output: 7 ('E')\n",
      "Step    2\n",
      "  input: 7 ('E')\n",
      "  expected output: 27 ('Y')\n",
      "Step    3\n",
      "  input: 27 ('Y')\n",
      "  expected output: 2 ('0')\n",
      "Step    4\n",
      "  input: 2 ('0')\n",
      "  expected output: 16 ('N')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eZKW6hte5MRy",
    "outputId": "5ee553fe-8352-4cc4-8eac-5f7bbe330b7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((32, 127), (32, 127)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 32 \n",
    "# 64 # 128 #256\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PKC6gXU15MR1"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "joOpyHRt5MR4"
   },
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUIcfB2T5MR5"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aOQgJMq5MR8"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XHJKCGRq5MR_",
    "outputId": "190b5686-e3e1-49f9-af79-d43672b416bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 127, 29) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "GXdzAAT-5MSC",
    "outputId": "93b431e6-3a89-48e9-b031-03270c784a46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (32, None, 256)           7424      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (32, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 29)            29725     \n",
      "=================================================================\n",
      "Total params: 3,975,453\n",
      "Trainable params: 3,975,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FW53TzGW5MSF"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "yw9HVb5F5MSI",
    "outputId": "3e665e88-8fb0-4e09-d627-145c37dfc0dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'DHAH0 DHAH0 YIH0R DHAH0S MAY0 YUW0NAH0VER0S SHEY0DIY0 WUH0D FRAH0NT DHAH0 AE0T FUH0L KAA0M DHAH0 \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'LK\\nUQGJVTMCY0\\nBSADIXPYLFIOVKFYPZU BN0ASOGYCMN JB\\nRUVIMJVRBRTTLMAJFQGNF\\nSYQSIXKHAJXBM IMZGPQGLQYOFMPFQXGLHZRKWZQQWLAXM0EBZZFRPVF'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "fOGGtRuP5MSL",
    "outputId": "8191cb51-8a69-4a61-c4d4-52c4d27e95e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (32, 127, 29)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       3.3743708\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8UfqzKx-5MSO"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cV1662AP5MSV"
   },
   "outputs": [],
   "source": [
    "#### ONLY RUN ON GOOGLE COLAB TO UTILZE GPU #####\n",
    "\n",
    "# # Directory where the checkpoints will be saved\n",
    "# checkpoint_dir = './training_checkpoints'\n",
    "# # Name of the checkpoint files\n",
    "# checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=checkpoint_prefix,\n",
    "#     save_weights_only=True)\n",
    "# EPOCHS=50\n",
    "# history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "# model.save('haiku_v3.h5') \n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')\n",
    "# model.save(\"/content/gdrive/My Drive/haiku_v3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "id": "f75DybTQ5MSh",
    "outputId": "39494f37-77d9-480e-baae-c62a6583289f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 256)            7424      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 29)             29725     \n",
      "=================================================================\n",
      "Total params: 3,975,453\n",
      "Trainable params: 3,975,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights('haiku_v2.h5')\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-SAaBdNZ5MSn"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = max_length\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the character returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted character as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Qp-g0BkfYZs"
   },
   "outputs": [],
   "source": [
    "with open(\"idict.json\", \"r\") as read_file:\n",
    "    idict = json.load(read_file)\n",
    "\n",
    "with open(\"pdict.json\", \"r\") as read_file:\n",
    "    pdict = json.load(read_file) \n",
    "rtransformer = ta.PhonemeReverse(pdict,idict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTjfZnea5MSp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cloudy',\n",
       " 'blue',\n",
       " 'oak',\n",
       " 'destroys',\n",
       " 'if',\n",
       " 'wouldst',\n",
       " 'tide',\n",
       " 'born',\n",
       " 'the',\n",
       " 'my',\n",
       " 'may',\n",
       " 'we']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform a seed into a phoneme\n",
    "inputs = rtransformer.convertsyllables(rtransformer.transform('cloudy')) # This function needs to be refactored to one\n",
    "# Use the phoneme to genereate a phoneme haiku\n",
    "output = generate_text(model, inputs)\n",
    "# Transform the phoneme into english\n",
    "rtransformer.getenglish(output,runs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the model is able to generate english, it is mostly incoherent. The ability to generate actual english is also partly because of unknown words being substitued with words generated from BERT - not due to the effectivness of the RNN model. Results are also inconsistent and the model didnt seem to learn that each output should be 17 syllables long. \n",
    "\n",
    "Next Steps:\n",
    "* Use all types of poems, not just haikus, and split them into 17 syllables in order to have a larger corpous.\n",
    "* Explore using other text generation tools such as GPT-2. \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "RNNModel.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
